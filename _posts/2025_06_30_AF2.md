---
title: "From AlphaFold2 to Boltz-2: A Journey Through the Revolution of Protein Structure Prediction"
author: "Aditya Sengar"
date: 2025-06-26
---

# Introduction: A New Era in Structural Biology

The last few years have seen a revolution in structural biology, driven by the breakneck speed of advances in artificial intelligence. This post is the first in a series where we will deconstruct the key architectures that have defined this new era. Our journey will take us from the deterministic brilliance of AlphaFold 2 to the generative power of modern diffusion models like Boltz-2.

To understand where the field is going, we must first build a deep appreciation for the foundation upon which it all stands.

**In this first article, we focus entirely on AlphaFold 2**, the model that solved one of biology's 50-year-old grand challenges: the protein folding problem. We will move beyond a high-level overview to dissect its two-part engine:

- **The Evoformer trunk**, which established a rich, _bidirectional dialogue_ between evolutionary data and a geometric hypothesis.
- The deterministic **Structure Module**, which used intricate, physically-inspired attention mechanisms to translate this hypothesis into a precise 3D structure.

By the end, you will have a solid, technically-grounded understanding of how AlphaFold 2 works, setting the stage for our exploration of the models that followed.

---

> **Wonder Box: The Basics—Proteins, Residues, and Evolution**
>
> **Proteins and Residues**  
> Think of a protein as a long, intricate necklace made of a specific sequence of beads. Each "bead" is an **amino acid**, which in this post is almost always called a **residue**. There are 20 common types of these residues, each with unique chemical properties.
>
> **The Protein Folding Problem**  
> The central challenge of structural biology is that this 1D string of residues folds itself into a single, complex, and functional 3D shape. For 50 years, predicting this final 3D shape from only the 1D sequence of residues was a grand challenge in science. This is the problem AlphaFold 2 solved.
>
> **MSA (Multiple Sequence Alignment)**  
> To get clues about the 3D shape, scientists look at the protein's "evolutionary cousins" found in other species. An **MSA** is a massive alignment of these related sequences. **Why is this important?** If two residues are touching in the 3D structure, a mutation in one is often compensated by a matching mutation in the other across many "cousin" sequences. By finding these correlated pairs of mutations (a signal called **co-evolution**), the model gets powerful hints about which residues are close to each other in the final structure.

---

Long before diffusion models entered the mainstream for structural biology, AlphaFold 2[^jumper2021nature] achieved a watershed moment. When DeepMind presented their results at the CASP14 competition in late 2020, they didn't just improve upon existing methods; they shattered all expectations, leaving the entire field of computational biology wondering how it was possible.[^service2020science]

When the paper and code were finally released, the "secret sauce" was revealed. It wasn't a single, magical insight into protein folding, but rather a masterpiece of deep learning[^outeiral2021opig] engineering that fundamentally reinvented how to reason about protein structure. The computational heart of this masterpiece consists of two main engines working in concert: the **Evoformer** and the **Structure Module**.

![High-level overview of the AlphaFold2 architecture. The model is conceptually divided into three stages. First, input features are generated from the primary sequence, a Multiple Sequence Alignment (MSA), and optional structural templates. Second, the **Evoformer** block iteratively refines an MSA representation and a pair representation in a deep communication network. Finally, the **Structure Module** translates the refined representations into the final 3D atomic coordinates.](AF2.png)
*Figure 1. High-level overview of AlphaFold2 architecture.*

---

## The Evoformer's Engine: The Attention Mechanism

The Evoformer's architecture is built on the now-ubiquitous **transformer**[^vaswani2017attention]. Its core computational tool is the **attention mechanism**, a powerful technique that allows the model to learn context by weighing the influence of different parts of the input data on each other (see Appendix for a numeric walk-through). When processing a specific residue, for example, it learns how much "attention" to pay to every other residue in the protein, allowing it to dynamically identify the most important relationships. For an excellent visual explanation of this concept, see Alammar[^alammar2018transformer].

## Building the Blueprint: The Representations and Templates

The self-attention mechanism is a powerful tool, but for it to work on a problem as complex as protein structure, it needs data representations that are far richer than simple word embeddings. For years, the state-of-the-art approach was to take a Multiple Sequence Alignment (MSA) and distill its evolutionary information down into a single, static 2D contact map[^contactmap]. Methods would analyze the MSA to predict which pairs of residues were likely touching, and this map of contacts would then be fed as restraints to a separate structure-folding algorithm.[^coevolution_slides]

AlphaFold 2’s paradigm shift was to **avoid this premature distillation**. Instead of collapsing the rich evolutionary data into a simple 2D map, it builds and refines two powerful representations in parallel, providing the perfect canvas for its attention-based Evoformer. Let’s look at how these representations are constructed.

### The MSA Representation: A Detailed Evolutionary Profile

AlphaFold 2 first clusters the MSA. The model then processes a representation of size $(N_{seq} \times N_{res})$, where $N_{seq}$ is the total number of sequences in the stack (composed of both MSA cluster representatives and structural templates) and $N_{res}$ is the number of residues.

For each residue in a representative sequence, the model starts with a rich, 49-dimensional input feature vector, including:
- **Residue identity** (one-hot encoding)
- **Local structural context** (deletion profile)
- **Evolutionary context** (cluster's statistical profile)

This 49-dimensional input vector is processed by a linear layer to produce the internal MSA representation, which has 256 channels ($c_m=256$). This is the representation that is iteratively refined within the Evoformer.

### The Pair Representation: The Geometric Blueprint

This is a square matrix of size $(N_{res} \times N_{res})$ with 128 channels ($c_z=128$) that evolves into a detailed geometric blueprint of the protein.

#### 1. Encoding Residue Pairs via Outer Sum

The initial pair information is created by simply adding two learned 128-dimensional vectors:
$$
\mathbf{z}_{ij}^{\text{pairs}} = \mathbf{a}_i + \mathbf{b}_j
$$

#### 2. Encoding Relative Position

The model injects 1D sequence separation:
$$
\mathbf{z}_{ij}^{\text{initial}} = \mathbf{z}_{ij}^{\text{pairs}} + \mathbf{p}_{ij}
$$
where $\mathbf{p}_{ij}$ is a positional embedding derived from the one-hot encoding of the clipped relative distance.

---

> **Wonder Box: Evoformer's Dialogue—A Summary**
>
> The core magic of the Evoformer lies in its two-way communication stream, repeated over 48 cycles:
> - **Path 1 (MSA → Pairs):** The model analyzes evolutionary data (MSA) to find correlated mutations, updating the geometric blueprint (Pair Representation).
> - **Path 2 (Pairs → MSA):** The model uses its current 3D hypothesis (Pair Representation) to guide its search through the evolutionary data.
>
> This cycle allows the model to build confidence by finding evidence consistent across both evolutionary and geometric domains.

---

### Structural Templates: A Powerful Head Start

If homologous structures[^homologous_structures] exist in the PDB, AlphaFold 2 leverages their known geometry as a set of powerful, editable hints:

- **2D Geometric Hint for the Pair Representation:** Extracts distograms[^distogram] and integrates them using a specialized attention mechanism.
- **1D "Advisor" for the MSA Representation:** Known backbone torsion angles are concatenated directly to the MSA representation as if they were additional sequences[^template_concat].

---

## The Evoformer's Work Cycle: A Refinement Loop

The Evoformer’s power comes from repeating a sophisticated block of operations 48 times. Each pass through this block represents one full cycle of the "dialogue" between the evolutionary and geometric representations.

### Stage 1: Processing the Evolutionary Data (MSA Stack)

Uses **row-wise** and **column-wise gated self-attention** for efficient inference, followed by an MLP (`MSATransition`).

### Stage 2: Enforcing Geometry (Pair Stack)

Uses **triangular multiplicative updates** and **triangular self-attention** to enforce geometric consistency:
- Multiplicative update:  
  $$
  \text{update\_vector} = \sum_{k} \left( \text{Linear}(\mathbf{z}_{ik}) \odot \text{Linear}(\mathbf{z}_{jk}) \right)
  $$
- Triangular self-attention:
  $$
  \text{score}_{ijk} = \frac{\mathbf{q}_{ij} \cdot \mathbf{k}_{ik}}{\sqrt{d_k}} + \text{Linear}(\mathbf{z}_{jk})
  $$
  $$
  \text{output}_{ij} = \mathbf{g}_{ij} \odot \sum_k \alpha_{ijk} \mathbf{v}_{ik}
  $$

### Stage 3: The Bidirectional Dialogue (Communication Hub)

#### Path 1: From MSA to Pairs (Outer Product Mean)
For each residue pair $(i, j)$:
$$
\text{update for } \mathbf{z}_{ij} = \text{Linear} \left( \text{mean}_{s} \left( \text{Linear}_a(\mathbf{m}_{si}) \otimes \text{Linear}_b(\mathbf{m}_{sj}) \right) \right)
$$

#### Path 2: From Pairs to MSA (Attention Bias)
$$
\text{score}(\mathbf{q}_{si}, \mathbf{k}_{sj}) = \frac{\mathbf{q}_{si} \cdot \mathbf{k}_{sj}}{\sqrt{d_k}} + \text{Linear}(\mathbf{z}_{ij})
$$

---

## The Structure Module: From Feature Maps to Atomic Coordinates

After 48 Evoformer iterations, the network possesses two mature tensors: a per-residue feature vector $\mathbf{s}_i$ and a per-pair tensor $\mathbf{z}_{ij}$. The **Structure Module** turns these into a 3D model using eight rounds of a custom transformer block called **Invariant Point Attention (IPA)**[^phie2023ipa], followed by a **Backbone Update**. The entire pipeline is **differentiable**[^end_to_end_diff].

![The AlphaFold2 Structure Module. (d) The module takes the final single and pair representations and uses an Invariant Point Attention (IPA) module to iteratively update a set of local reference frames for each residue. (e) These local frames define the orientation of each amino acid. (f) The final output is a complete 3D structure, shown here superimposed on the ground truth.](structure_module.png)
*Figure 2. AlphaFold2 Structure Module.*

---

> **Wonder Box: The Structure Module's Philosophy—Frames, Invariance, and Refinement**
>
> - **Local Frames:** Each residue gets its own local coordinate system.
> - **Invariant Point Attention (IPA):** Combines chemical/sequence similarity, geometric blueprint, and current 3D proximity.
> - **Iterative Refinement:** The final structure is refined over 8 IPA cycles, and the prediction process can be "recycled" multiple times for further accuracy.

---

### Local Frames

Every residue $i$ is assigned an **internal frame** $T_i = (R_i, \mathbf{t}_i)$:
- $R_i$ is a rotation matrix in $SO(3)$.
- $\mathbf{t}_i$ is an origin in $\mathbb{R}^3$.

All frames start with the identity transformation ("black hole initialization").  
Advantages:
1. Global rigid-body motions don't affect internal distances.
2. Allows independent application of rotations and translations per residue.

### What Makes IPA “Invariant”?

Each residue advertises $K$ learnable **query points** $\mathbf{p}_{i,k}$ in its local frame.  
Global positions:
$$
\tilde{\mathbf{p}}_{i,k} = R_i \mathbf{p}_{i,k} + \mathbf{t}_i
$$
Distance:
$$
d_{ij,k\ell} = \lVert \tilde{\mathbf{p}}_{i,k} - \tilde{\mathbf{p}}_{j,\ell} \rVert_2
$$

### Scoring Who to Attend To

Combined score:
$$
\mathrm{score}^{(h)}_{ij} = {\mathbf{q}^{(h)}_i}^{\mathsf{T}}\mathbf{k}^{(h)}_j + b^{(h)}_{ij} - \frac{1}{\sigma_h^2}\sum_{k,\ell} w^{(h)}_{k\ell} d_{ij,k\ell}^2
$$

Weighted sums yield the updated representations and backbone shift/rotation.  
Backbone update is performed via a small MLP head predicting translation and an axis-angle rotation vector $\boldsymbol\omega_i$, which is converted to a rotation matrix using the **exponential map**[^exp_map]:

$$
\delta R_i = \exp(\boldsymbol\omega_i)
$$

Frame update:
$$
R_i \leftarrow \delta R_i R_i \\
t_i \leftarrow \delta R_i t_i + \delta t_i
$$

Repeat for 8 cycles.

---

> **Wonder Box: Why this Design Works**
>
> IPA gives the network three complementary signals for deciding proximity: biochemical compatibility (query–key), Evoformer’s geometric experience (bias), and current 3D structure (distance). Gating in both attention and the backbone update lets the model ignore unhelpful suggestions, ensuring stability.

---

### Side-chain Placement

Final MLP heads predict side-chain conformations. The network predicts up to four torsion angles per residue, using $\sin$ and $\cos$ for smoothness, and special loss logic for symmetric residues.

All heavy atoms are placed with idealized bond lengths/angles, and optionally relaxed using a force field (e.g., AMBER)[^relaxation].

---

## The Training Objective: A Symphony of Losses

> **Wonder Box: Measuring Success—Key Metrics & Loss Functions**
>
> - **RMSD (Root Mean Square Deviation):** Superimpose predicted and true structures, measure average atom distance.
> - **FAPE (Frame Aligned Point Error):** Measures atomic error in each residue’s local frame.
> - **Distogram:** A 2D histogram over pairwise distances, learned as an auxiliary output.

AlphaFold 2 is trained with a **weighted sum of several loss components**:
- **FAPE loss** on the backbone coordinates
- **Distogram loss** (encourages accurate pair representations)
- **MSA masking loss** (predicts masked amino acids)
- **Predicted confidence metrics**: pLDDT (local), PAE (domain), and pTM-score (global)

This combination of objectives trains the model to understand evolutionary context, build accurate 3D models, and assess its own confidence.

---

## A Summary of the Revolution

Ultimately, the genius of AlphaFold 2 lies in its integrated design. It doesn't treat protein structure prediction as a simple pipeline, but as a holistic reasoning problem. The Evoformer creates a rich, context-aware blueprint by forcing a deep dialogue between evolutionary data and a geometric hypothesis. The Structure Module then uses a physically-inspired, equivariant attention mechanism to translate this abstract blueprint into a precise atomic model. This end-to-end philosophy, guided by a symphony of carefully chosen loss functions, is what allowed AlphaFold 2 to not just advance the field, but to fundamentally redefine what was thought possible.

---

# Appendix: A Deeper Look at Self-Attention

The core computational engine of the Transformer architecture is the **self-attention mechanism**. Its purpose is to generate a new, contextually-aware representation for each element in an input sequence.

**Example**: Processing a peptide of Alanine, Lysine, and Aspartate.

- **Input Embeddings**: Each amino acid represented as a learned vector.

$$
\mathbf{X} =
\begin{bmatrix}
0.2 & 0.1 & 0.0 & 0.0 \\\\
0.8 & 0.4 & 1.0 & 0.0 \\\\
0.5 & 0.3 & 0.0 & 1.0
\end{bmatrix}
$$

- **Project to Query, Key, Value**:  
  Three matrices project input vectors to $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ spaces.

$$
\mathbf{W}_Q = \begin{bmatrix} 0 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \end{bmatrix}, \ 
\mathbf{W}_K = \begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 1 & 0 \end{bmatrix}, \
\mathbf{W}_V = \begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \end{bmatrix}
$$

- **Compute Attention Scores**:

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q} \cdot \mathbf{k}
$$

- **Scale and Softmax**:

Scores are divided by $\sqrt{d_k}$ and softmaxed to get weights.

- **Output**: Weighted sum of values gives new context-aware representations.

---

### Multi-Head Attention

Multi-head attention lets the model consider multiple relationship types in parallel. Outputs from all heads are concatenated and linearly projected for the final representation.

---

# References

[^service2020science]: Service, R. F. (2020). ‘The game has changed.’ AI triumphs at solving protein structures. *Science*.
[^outeiral2021opig]: Outeiral Rubiera, C. (2021). AlphaFold 2 is here: what’s behind the structure prediction miracle. *Oxford Protein Informatics Group Blog*. [link](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)
[^jumper2021nature]: Jumper, J., Evans, R., Pritzel, A., et al. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature*, 596(7873):583–589.
[^vaswani2017attention]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems*, 30.
[^alammar2018transformer]: Alammar, J. (2018). The Illustrated Transformer. [Blog Post](http://jalammar.github.io/illustrated-transformer/).
[^coevolution_slides]: Ovchinnikov, S. (2018). Co-evolutionary methods for protein structure prediction. *Stanford CS371 Lecture Slides*. [link](https://cs371.stanford.edu/2018_slides/coevolution.pdf)
[^phie2023ipa]: Phie, K. (2023). Invariant Point Attention explained. *Medium*. [link](https://medium.com/@kasothaphie/invariant-point-attention-explained-c71aa56f5f5c)
[^contactmap]: A contact map is a 2D matrix where entry $(i, j)$ represents if residue $i$ and residue $j$ are in physical contact (e.g., within 8 Å).
[^homologous_structures]: Homologous structures are experimentally solved 3D structures from proteins that are evolutionarily related to the target protein.
[^distogram]: A distogram is a matrix where entry $(i,j)$ is a probability distribution over distance bins (e.g., 39 bins from 3.25 Å to >50.75 Å).
[^template_concat]: Template torsion angle features are stacked with the main MSA tensor along the sequence dimension.
[^exp_map]: The exponential map converts an axis-angle vector into a valid rotation matrix.
[^end_to_end_diff]: End-to-end differentiable means every model parameter can be adjusted directly from the prediction error, all the way from final output to the start.
[^relaxation]: An optional final energy minimization step (e.g., with AMBER) can be used to resolve atomic clashes, but has little effect on AlphaFold’s confidence scores.

