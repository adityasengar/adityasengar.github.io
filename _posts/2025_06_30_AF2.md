---
title: "From AlphaFold2 to Boltz-2: A Journey Through the Revolution of Protein Structure Prediction"
author: "Aditya Sengar"
date: 2025-06-26
---

# Introduction: A New Era in Structural Biology

The last few years have seen a revolution in structural biology, driven by the breakneck speed of advances in artificial intelligence. This post is the first in a series where we will deconstruct the key architectures that have defined this new era. Our journey will take us from the deterministic brilliance of AlphaFold 2 to the generative power of modern diffusion models like Boltz-2.

To understand where the field is going, we must first build a deep appreciation for the foundation upon which it all stands.

**In this first article, we focus entirely on AlphaFold 2**, the model that solved one of biology's 50-year-old grand challenges: the protein folding problem. We will move beyond a high-level overview to dissect its two-part engine:

- The **Evoformer** trunk, which established a rich, bidirectional dialogue between evolutionary data and a geometric hypothesis.
- The deterministic **Structure Module**, which used intricate, physically-inspired attention mechanisms to translate this hypothesis into a precise 3D structure.

By the end, you will have a solid, technically-grounded understanding of how AlphaFold 2 works, setting the stage for our exploration of the models that followed.

---

> ### The Basics—Proteins, Residues, and Evolution
>
> **Proteins and Residues**  
> Think of a protein as a long, intricate necklace made of a specific sequence of beads. Each "bead" is an amino acid, which in this post is almost always called a residue. There are 20 common types of these residues, each with unique chemical properties.
>
> **The Protein Folding Problem**  
> The central challenge of structural biology is that this 1D string of residues folds itself into a single, complex, and functional 3D shape. For 50 years, predicting this final 3D shape from only the 1D sequence of residues was a grand challenge in science. This is the problem AlphaFold 2 solved.
>
> **MSA (Multiple Sequence Alignment)**  
> To get clues about the 3D shape, scientists look at the protein's "evolutionary cousins" found in other species. An MSA is a massive alignment of these related sequences. Why is this important? If two residues are touching in the 3D structure, a mutation in one is often compensated by a matching mutation in the other across many "cousin" sequences. By finding these correlated pairs of mutations (a signal called co-evolution), the model gets powerful hints about which residues are close to each other in the final structure.

---

Long before diffusion models entered the mainstream for structural biology, AlphaFold 2[^jumper2021nature] achieved a watershed moment. When DeepMind presented their results at the CASP14 competition in late 2020, they didn't just improve upon existing methods; they shattered all expectations, leaving the entire field of computational biology wondering how it was possible.[^service2020science]

When the paper and code were finally released, the "secret sauce" was revealed. It wasn't a single, magical insight into protein folding, but rather a masterpiece of deep learning[^outeiral2021opig] engineering that fundamentally reinvented how to reason about protein structure. The computational heart of this masterpiece consists of two main engines working in concert: the **Evoformer** and the **Structure Module**.

![High-level overview of the AlphaFold2 architecture. The model is conceptually divided into three stages. First, input features are generated from the primary sequence, a Multiple Sequence Alignment (MSA), and optional structural templates. Second, the Evoformer block iteratively refines an MSA representation and a pair representation in a deep communication network. Finally, the Structure Module translates the refined representations into the final 3D atomic coordinates.](AF2.png)
*Figure 1. High-level overview of AlphaFold2 architecture.*

---

## The Evoformer's Engine: The Attention Mechanism

The Evoformer's architecture is built on the now-ubiquitous transformer[^vaswani2017attention]. Its core computational tool is the attention mechanism, a powerful technique that allows the model to learn context by weighing the influence of different parts of the input data on each other (see Appendix for a numeric walk-through). When processing a specific residue, for example, it learns how much "attention" to pay to every other residue in the protein, allowing it to dynamically identify the most important relationships. For an excellent visual explanation of this concept, see Alammar[^alammar2018transformer].

## Building the Blueprint: The Representations and Templates

The self-attention mechanism is a powerful tool, but for it to work on a problem as complex as protein structure, it needs data representations that are far richer than simple word embeddings. For years, the state-of-the-art approach was to take a Multiple Sequence Alignment (MSA) and distill its evolutionary information down into a single, static 2D contact map[^contactmap]. Methods would analyze the MSA to predict which pairs of residues were likely touching, and this map of contacts would then be fed as restraints to a separate structure-folding algorithm.[^coevolution_slides]

AlphaFold 2’s paradigm shift was to avoid this premature distillation. Instead of collapsing the rich evolutionary data into a simple 2D map, it builds and refines two powerful representations in parallel, providing the perfect canvas for its attention-based Evoformer. Let’s look at how these representations are constructed.

### The MSA Representation: A Detailed Evolutionary Profile

AlphaFold 2 first clusters the MSA. The model then processes a representation of size ($N_{seq}$ × $N_{res}$), where $N_{seq}$ is the total number of sequences in the stack (composed of both MSA cluster representatives and structural templates) and N_res is the number of residues.

The key is that for each residue in a representative sequence, the model starts with a rich, 49-dimensional input feature vector. To make this concrete, imagine the model is looking at residue 50 of a representative sequence, which is a Leucine (L). Its input vector would answer several questions simultaneously:

- **What is this residue?**  
  A set of channels acts as a one-hot encoding, where the channel for "Leucine" is set to 1 and all others are 0.

- **What is the local structural context? The Deletion Profile.**  
  Other input channels track a deletion profile from the MSA, answering the question: "Across all related sequences, how often is a residue missing at this spot?" This provides a powerful structural hint, as a high frequency of deletions suggests a flexible loop, while a low frequency points to the protein's stable, conserved core.

- **What is its evolutionary context?**  
  A large portion of the vector is dedicated to the cluster's statistical profile. This profile summarizes all the sequences in that family branch, saying something like: *"For position 50, Leucine appears 70% of the time, Isoleucine appears 20%, and Valine appears 5%."*

This 49-dimensional input vector is then processed by a linear layer to produce the internal MSA representation, which has 256 channels ($c_{m}=256$). This is the representation that is iteratively refined within the Evoformer.




### The Pair Representation: The Geometric Blueprint

This is a square matrix of size ($N_{\text{res}} \times N_{\text{res}}$) with 128 channels ($c_{z}=128$) that evolves into a detailed geometric blueprint of the protein. It's the model's internal hypothesis about the 3D structure. It doesn't start as a blank slate; the initial representation $z_{ij}$ is constructed from the primary sequence itself, seeding the model with fundamental information about residue identity and position. This is done in two steps:

#### 1. Encoding Residue Pairs via Outer Sum

First, the model learns two separate 128-dimensional vector representations, $a_{i}$ and $b_{j}$, for each amino acid at positions $i$ and $j$. These are learned by passing the one-hot encoding of the amino acid type through two different linear layers. The initial pair information is then created by simply adding these vectors:

$$
z_{ij}^{\text{pairs}} = a_{i} + b_{j}
$$

This operation creates a basic ($N_{\text{res}} \times N_{\text{res}}$) map where each entry is a 128-dimensional vector influenced by the specific amino acids at positions $i$ and $j$.

#### 2. Encoding Relative Position

Next, the model explicitly injects information about the 1D sequence separation. It calculates the relative distance $d_{ij} = i - j$, which is clipped to a fixed window (e.g., between $-32$ and $+32$) and converted into a one-hot vector. This binary vector is then passed through another linear layer to create a dedicated 128-dimensional positional embedding, $p_{ij}$.

The final initial pair representation is the sum of these two components:

$$
z_{ij}^{\text{initial}} = z_{ij}^{\text{pairs}} + p_{ij}
$$

This initial grid is like digital graph paper, with basic 1D relationships sketched out. The job of the Evoformer is to enrich this grid, filling its 128 channels with sophisticated 3D information like residue-pair distances and orientations.



### Structural Templates: A Powerful Head Start

The third, optional source of information comes from **structural templates**. If homologous structures[^homologous_structures] exist in the PDB, AlphaFold 2 leverages their known geometry not as a rigid scaffold, but as a set of powerful, editable hints. This information is integrated through two distinct and sophisticated pathways:

#### 1. A 2D Geometric Hint for the Pair Representation

First, the model extracts key geometric features from each template, most notably a **distogram**[^distogram]—a detailed map of distances between all pairs of residues. This collection of geometric data is then processed by its own dedicated "Template Pair Stack." This stack refines the template information using a simplified version of the powerful **triangular updates**, a core mechanism of the Evoformer that we will deconstruct shortly.

The model then integrates this refined template data into its main geometric blueprint using a clever attention process. For every residue pair $(i, j)$, a specialized attention mechanism assesses the geometric hints provided by all available templates. It effectively asks a crucial question: *"For this specific pair, which of my templates offers the most reliable geometric clue?"* Based on the answer, it calculates a weighted average of the geometric information from all templates and adds this "consensus hint" directly to its own evolving blueprint. This process allows the model to intelligently fuse information, for instance, by trusting one template for a local helix while relying on another for the orientation of a distant domain.

#### 2. A 1D "Advisor" for the MSA Representation

Second, in a particularly elegant move, the model extracts the known backbone torsion angles from the templates. These angles are embedded into a feature vector and then **concatenated directly to the MSA representation** as if they were additional sequences[^template_concat]. This treats the template not as a static map, but as an expert participant in the evolutionary dialogue. By sitting alongside the other sequences, its structural information can directly bias the MSA attention mechanisms. For example, if a template's torsion angles clearly define a beta-strand, it can encourage the MSA attention to focus on finding the long-range co-evolutionary signals that are characteristic of beta-sheet formation.

[^homologous_structures]: Homologous structures are experimentally solved 3D structures from proteins that are evolutionarily related to the target protein. Because they share a common ancestor, their overall 3D folds are often highly similar, making them excellent starting points or 'templates' for prediction.

[^distogram]: A distogram is a matrix where the entry for a residue pair $(i, j)$ is a full probability distribution over a set of distance bins (in AlphaFold 2, 39 bins from 3.25 Å to over 50.75 Å). As an interesting developmental note, the original design also extracted the relative 3D orientation of residues, but the authors found this feature was not essential and disabled it in the final models, indicating the distogram provided the most critical geometric signal.

[^template_concat]: To be precise, the template torsion angle features are processed to have the same shape as the MSA representation ($N_{\text{templ}} \times N_{\text{res}} \times c_{m}$). This tensor is then stacked with the main MSA tensor ($M_{\text{msa}}$) along the sequence dimension, creating a final, larger tensor that the Evoformer processes.






## The Evoformer's Work Cycle: A Refinement Loop

The Evoformer’s power comes from repeating a sophisticated block of operations 48 times. Each pass through this block represents one full cycle of the "dialogue" between the evolutionary and geometric representations.

---

> ### Evoformer's Dialogue—A Summary
>
> The core magic of the Evoformer lies in its two-way communication stream, repeated over 48 cycles. Think of it as a feedback loop:
>
> - **Path 1 (MSA $\rightarrow$ Pairs):** The model analyzes the evolutionary data (MSA) to find correlated mutations. It summarizes this evidence and sends it to the geometric blueprint (Pair Representation), saying: “Here are the residue pairs that are evolutionarily linked. You should probably move them closer together.”
>
> - **Path 2 (Pairs $\rightarrow$ MSA):** The model then uses its current 3D hypothesis (Pair Representation) to guide its search through the evolutionary data. It tells the MSA attention mechanism: “I have strong geometric evidence that these two residues are close. Pay extra attention to any co-evolution signal between them, no matter how subtle.”
>
> This cycle allows the model to build confidence by finding evidence that is consistent across both the evolutionary and geometric domains.
---

### Stage 1: Processing the Evolutionary Data (MSA Stack)

Uses row-wise and column-wise gated self-attention for efficient inference, followed by an MLP (MSATransition).

### Stage 2: Enforcing Geometry (Pair Stack)

Uses triangular multiplicative updates and triangular self-attention to enforce geometric consistency:

Multiplicative update:  
$$
\text{update\_vector} = \sum_{k} \left( \text{Linear}(z_{ik}) \odot \text{Linear}(z_{jk}) \right)
$$

Triangular self-attention:
$$
\text{score}_{ijk} = \frac{q_{ij} \cdot k_{ik}}{\sqrt{d_k}} + \text{Linear}(z_{jk})
$$

$$
\text{output}_{ij} = g_{ij} \odot \sum_k \alpha_{ijk} v_{ik}
$$

### Stage 3: The Bidirectional Dialogue (Communication Hub)

#### Path 1: From MSA to Pairs (Outer Product Mean)
For each residue pair (i, j):
$$
\text{update for } z_{ij} = \text{Linear} \left( \text{mean}_{s} \left( \text{Linear}_a(m_{si}) \otimes \text{Linear}_b(m_{sj}) \right) \right)
$$

#### Path 2: From Pairs to MSA (Attention Bias)
$$
\text{score}(q_{si}, k_{sj}) = \frac{q_{si} \cdot k_{sj}}{\sqrt{d_k}} + \text{Linear}(z_{ij})
$$

---

## The Structure Module: From Feature Maps to Atomic Coordinates

After 48 Evoformer iterations, the network possesses two mature tensors: a per-residue feature vector $s_i$ and a per-pair tensor $z_{ij}$. The **Structure Module** turns these into a 3D model using eight rounds of a custom transformer block called **Invariant Point Attention (IPA)**[^phie2023ipa], followed by a **Backbone Update**. The entire pipeline is differentiable[^end_to_end_diff].

![The AlphaFold2 Structure Module. (d) The module takes the final single and pair representations and uses an Invariant Point Attention (IPA) module to iteratively update a set of local reference frames for each residue. (e) These local frames define the orientation of each amino acid. (f) The final output is a complete 3D structure, shown here superimposed on the ground truth.](structure_module.png)
*Figure 2. AlphaFold2 Structure Module.*

---

> ### Wonder Box: The Structure Module's Philosophy—Frames, Invariance, and Refinement
>
> - **Local Frames:** Each residue gets its own local coordinate system.
> - **Invariant Point Attention (IPA):** Combines chemical/sequence similarity, geometric blueprint, and current 3D proximity.
> - **Iterative Refinement:** The final structure is refined over 8 IPA cycles, and the prediction process can be "recycled" multiple times for further accuracy.

---

### Local Frames

Every residue $i$ is assigned an internal frame $T_i = (R_i, t_i)$:
- $R_i$ is a rotation matrix in SO(3).
- $t_i$ is an origin in $\mathbb{R}^3$.

All frames start with the identity transformation ("black hole initialization").

**Advantages:**
1. Global rigid-body motions don't affect internal distances.
2. Allows independent application of rotations and translations per residue.

### What Makes IPA “Invariant”?

Each residue advertises $K$ learnable query points $p_{i,k}$ in its local frame.  
Global positions:
$$
\tilde{p}_{i,k} = R_i p_{i,k} + t_i
$$
Distance:
$$
d_{ij,k\ell} = \lVert \tilde{p}_{i,k} - \tilde{p}_{j,\ell} \rVert_2
$$

### Scoring Who to Attend To

Combined score:
$$
\text{score}^{(h)}_{ij} = q^{(h)}_i{}^{T} k^{(h)}_j + b^{(h)}_{ij} - \frac{1}{\sigma_h^2}\sum_{k,\ell} w^{(h)}_{k\ell} d_{ij,k\ell}^2
$$

Weighted sums yield the updated representations and backbone shift/rotation.  
Backbone update is performed via a small MLP head predicting translation and an axis-angle rotation vector $\omega_i$, which is converted to a rotation matrix using the exponential map[^exp_map]:

$$
\delta R_i = \exp(\omega_i)
$$

Frame update:
$$
R_i \leftarrow \delta R_i R_i \\
t_i \leftarrow \delta R_i t_i + \delta t_i
$$

Repeat for 8 cycles.

---

> ### Wonder Box: Why this Design Works
>
> IPA gives the network three complementary signals for deciding proximity: biochemical compatibility (query–key), Evoformer’s geometric experience (bias), and current 3D structure (distance). Gating in both attention and the backbone update lets the model ignore unhelpful suggestions, ensuring stability.

---

### Side-chain Placement

Final MLP heads predict side-chain conformations. The network predicts up to four torsion angles per residue, using $\sin$ and $\cos$ for smoothness, and special loss logic for symmetric residues.

All heavy atoms are placed with idealized bond lengths/angles, and optionally relaxed using a force field (e.g., AMBER)[^relaxation].

---

## The Training Objective: A Symphony of Losses

> ### Wonder Box: Measuring Success—Key Metrics & Loss Functions
>
> - **RMSD (Root Mean Square Deviation):** Superimpose predicted and true structures, measure average atom distance.
> - **FAPE (Frame Aligned Point Error):** Measures atomic error in each residue’s local frame.
> - **Distogram:** A 2D histogram over pairwise distances, learned as an auxiliary output.

AlphaFold 2 is trained with a weighted sum of several loss components:
- **FAPE loss** on the backbone coordinates
- **Distogram loss** (encourages accurate pair representations)
- **MSA masking loss** (predicts masked amino acids)
- **Predicted confidence metrics**: pLDDT (local), PAE (domain), and pTM-score (global)

This combination of objectives trains the model to understand evolutionary context, build accurate 3D models, and assess its own confidence.

---

## A Summary of the Revolution

Ultimately, the genius of AlphaFold 2 lies in its integrated design. It doesn't treat protein structure prediction as a simple pipeline, but as a holistic reasoning problem. The Evoformer creates a rich, context-aware blueprint by forcing a deep dialogue between evolutionary data and a geometric hypothesis. The Structure Module then uses a physically-inspired, equivariant attention mechanism to translate this abstract blueprint into a precise atomic model. This end-to-end philosophy, guided by a symphony of carefully chosen loss functions, is what allowed AlphaFold 2 to not just advance the field, but to fundamentally redefine what was thought possible.

---

# Appendix: A Deeper Look at Self-Attention

The core computational engine of the Transformer architecture is the self-attention mechanism. Its purpose is to generate a new, contextually-aware representation for each element in an input sequence.

**Example**: Processing a peptide of Alanine, Lysine, and Aspartate.

- **Input Embeddings**: Each amino acid represented as a learned vector.

$$
X =
\begin{bmatrix}
0.2 & 0.1 & 0.0 & 0.0 \\\\
0.8 & 0.4 & 1.0 & 0.0 \\\\
0.5 & 0.3 & 0.0 & 1.0
\end{bmatrix}
$$

- **Project to Query, Key, Value**:  
  Three matrices project input vectors to $Q$, $K$, $V$ spaces.

$$
W_Q = \begin{bmatrix} 0 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \end{bmatrix}, \ 
W_K = \begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 1 & 0 \end{bmatrix}, \
W_V = \begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \end{bmatrix}
$$

- **Compute Attention Scores**:

$$
\text{score}(q, k) = q \cdot k
$$

- **Scale and Softmax**:

Scores are divided by $\sqrt{d_k}$ and softmaxed to get weights.

- **Output**: Weighted sum of values gives new context-aware representations.

---

### Multi-Head Attention

Multi-head attention lets the model consider multiple relationship types in parallel. Outputs from all heads are concatenated and linearly projected for the final representation.

---

# References

[^service2020science]: Service, R. F. (2020). ‘The game has changed.’ AI triumphs at solving protein structures. *Science*.
[^outeiral2021opig]: Outeiral Rubiera, C. (2021). AlphaFold 2 is here: what’s behind the structure prediction miracle. *Oxford Protein Informatics Group Blog*. [link](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)
[^jumper2021nature]: Jumper, J., Evans, R., Pritzel, A., et al. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature*, 596(7873):583–589.
[^vaswani2017attention]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems*, 30.
[^alammar2018transformer]: Alammar, J. (2018). The Illustrated Transformer. [Blog Post](http://jalammar.github.io/illustrated-transformer/).
[^coevolution_slides]: Ovchinnikov, S. (2018). Co-evolutionary methods for protein structure prediction. *Stanford CS371 Lecture Slides*. [link](https://cs371.stanford.edu/2018_slides/coevolution.pdf)
[^phie2023ipa]: Phie, K. (2023). Invariant Point Attention explained. *Medium*. [link](https://medium.com/@kasothaphie/invariant-point-attention-explained-c71aa56f5f5c)
[^contactmap]: A contact map is a 2D matrix where entry (i, j) represents if residue i and residue j are in physical contact (e.g., within 8 Å).
[^homologous_structures]: Homologous structures are experimentally solved 3D structures from proteins that are evolutionarily related to the target protein.
[^distogram]: A distogram is a matrix where entry (i,j) is a probability distribution over distance bins (e.g., 39 bins from 3.25 Å to >50.75 Å).
[^template_concat]: Template torsion angle features are stacked with the main MSA tensor along the sequence dimension.
[^exp_map]: The exponential map converts an axis-angle vector into a valid rotation matrix.
[^end_to_end_diff]: End-to-end differentiable means every model parameter can be adjusted directly from the prediction error, all the way from final output to the start.
[^relaxation]: An optional final energy minimization step (e.g., with AMBER) can be used to resolve atomic clashes, but has little effect on AlphaFold’s confidence scores.

