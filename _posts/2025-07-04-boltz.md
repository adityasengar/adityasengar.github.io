---
title: "From Fold to Function: Inside Boltz"
series: "From AlphaFold2 to Boltz-2: The Protein Prediction Revolution"
author: "Aditya Sengar"
part: 3
date: 2025-07-04
---
# From Co-Folding to Co-Design: The Definitive Deep Dive into the Boltz-1 and Boltz-2 Models


## Introduction

In the first part of this series[[^sengarAF2blog]], we saw how AlphaFold 2 conquered a 50-year-old grand challenge. In the second[[^sengarAF3blog]], we watched AlphaFold 3 take a generative leap, moving from single proteins to entire molecular symphonies. These were monumental achievements, but they came with a catch: the most advanced model, AlphaFold 3, remained behind a closed door, limiting its use for widespread research and development.

In late 2024, a team at MIT made a bold move by releasing **Boltz-1**[[^boltz1]], the first fully open-source model approaching the prowess of AlphaFold 3. This was more than just a reimplementation; it was a statement. AlphaFold 3 had demonstrated that *generative diffusion* models could tackle multi-molecule structure prediction, but its closed license limited widespread use. Boltz-1 shattered that barrier, quickly becoming one of the most adopted *co-folding* models in industry. Within weeks, companies like Recursion were reportedly integrating Boltz-1 into their drug discovery pipelines, proving the immense demand for accessible, state-of-the-art AI.

But Boltz-1 was just the opening act. In mid-2025, its successor, Boltz-2, arrived and changed the game entirely[[^boltz2]]. It pushed beyond just predicting structures and into the realm of predicting function. The name "Boltz" itself is a nod to Boltzmann and the physics of thermodynamics—a fitting choice for a model that not only asks "how do these molecules fit?" but also "how tightly will they hold on?" **Boltz-2** is heralded as the first AI model to approach the accuracy of rigorous free-energy perturbation (FEP) simulations while operating over 1000x faster.[^fep] In practical terms, what used to take 6–12 hours of computation can now be achieved in seconds on a single GPU [[^bioit_boltz2]]. This leap enables researchers to screen thousands of drug candidates virtually and rapidly zero in on the most promising ones — an advancement poised to cut down drug development timelines.

This post will dissect the Boltz-1 and Boltz-2 models, framing them not as mere clones, but as the next chapter in the protein prediction revolution—a chapter written by and for the open-source community. We'll explore how they refined the AlphaFold 3 blueprint and then forged into new territory, tackling the holy grail of drug discovery: binding affinity. Specifically, we will explore four key areas of innovation:
* **Architectural Foundations and Efficiency:** How Boltz-1 replicated and optimized the AlphaFold 3 architecture, introducing tricks like *TriFast* attention kernels[[^trifast]] to enable larger complexes and faster inference.
* **The Diffusion Pipeline:** A deep dive into the core generative process and the critical innovations (like per-step Kabsch alignment) that the Boltz models introduced to enhance stability and physical realism.
* **Physical Realism and Control:** How Boltz-2 addresses lingering physical issues of generative models (steric clashes, chain tangling, chirality errors) with *Boltz-Steering* potentials, and empowers users with new controls like template enforcement and custom restraints.
* **Binding Affinity Prediction:** How Boltz-2 expands its mission from predicting structures to also predicting how strongly two molecules bind, via a novel affinity head trained on massive biochemical datasets.




> **TL;DR: Boltz-2 – Open-Source AI for Structure & Binding Affinity**
>
> **Boltz-1** is an open-source reimplementation of AlphaFold 3 that predicts the 3D structure of protein complexes (and protein–ligand interactions) with AlphaFold3-level accuracy. It adopts AlphaFold 3’s two-stage strategy: a **Pairformer** trunk that builds a detailed geometric blueprint of all interacting molecules, and a diffusion-based generative module that co-folds the complex. Boltz-1 introduced efficiency improvements (like *TriFast* fused attention kernels) and made multiple small architectural tweaks for stability and logic, all while being freely available under an MIT license. The open release of Boltz-1 has dramatically lowered barriers to applying state-of-the-art structure prediction in practice.
>
> **Boltz-2** goes a step further, transforming the co-folding model into a *co-design* model. It adds a suite of **physics-inspired “steering” potentials** that gently guide the generative process to produce physically realistic models (no more distorted bond angles or left-handed amino acids). It also gives users more control—you can bias the model with known structures (templates), enforce custom distance restraints, or even tell it to mimic experimental conditions (e.g., `--method cryo-EM` for softer, more flexible outputs). Most remarkably, Boltz-2 introduces an **affinity prediction head** that estimates how strongly a ligand will bind its protein target. This head was trained on over a million experimental binding measurements and approaches the accuracy of gold-standard free energy calculations (which take days) in mere seconds. In short, Boltz-2 doesn’t just predict how molecules will fit together—it predicts how tightly they’ll hold on, bringing us closer to AI-driven drug design.

> **Wonder Box: From Co-Folding to “Co-Design”**
>
> In traditional *co-folding*, an AI model predicts how multiple pieces (e.g., two proteins, or a protein and a drug) come together into a complex structure — much like solving a 3D puzzle. With **Boltz-2**, we're elevating to *co-design*: not only does the model assemble the puzzle, it also evaluates how good the fit is. This is akin to not just drawing two jigsaw pieces that lock together, but also estimating *how strong that lock is* (will the pieces stay together or fall apart easily?). In practical terms, co-design means the AI helps suggest which molecules might *work best* together, not just how they might look together.

***

> #A Bridge from Part 2: From a Closed Model to an Open Toolkit
>
> In Part 2, we witnessed a fundamental paradigm shift with AlphaFold 3. The focus expanded from single proteins to the complex dance of all of life's molecules.
>
> Let's remember the key innovations that made this possible:
>
> **Universal Language**: The move from protein-specific "residues" to universal "tokens" allowed the model to process proteins, DNA, RNA, and ligands in a unified framework.
>
> **Generative Diffusion**: We traded AlphaFold 2's deterministic builder for a generative "sculptor." This new engine could start with a random cloud of atoms and, guided by a blueprint from the Pairformer, denoise it into a coherent multi-molecule structure.
>
> AlphaFold 3 proved that a generative approach could work wonders, but its revolutionary power remained behind a closed door. This is where our story takes a turn—from a breakthrough revealed by a single company to one embraced and advanced by an open community.
>
> In this post, we'll see how the Boltz models took the powerful generative blueprint of AlphaFold 3 and not only democratized it but also taught it the laws of physics and the economics of drug discovery. We're moving beyond just plausible structures to ones that are physically realistic, and from asking "How do these fit?" to finally answering, "And how tightly will they hold on?"

## The Foundational Architecture: A Better, Faster, and More Open Engine

![Architectural comparison of Boltz vs AlphaFold 3.](/images/Boltz_AF3.png)
Figure 1. Schematic comparison of AlphaFold 3 (top) and Boltz-2 (bottom). Boltz-2 adds depth to the trunk (64 vs 48 layers), swaps in TriFast kernels for speed, and introduces dedicated confidence and affinity heads. Adapted from Passaro et al. 2025 [[^boltz2]]. Architecturally, Boltz-2 is almost a drop-in replacement for Boltz-1; the major new ingredient is a specialised affinity head that brings free-energy-perturbation-level binding predictions into the same network.


At its core, Boltz-1 adopted the powerful blueprint pioneered by AlphaFold 3 (see Part 2 [[^sengarAF3blog]]): a deep learning system composed of a **Pairformer** trunk to learn rich representations of the entire molecular system, and a **diffusion-based generator** to translate those representations into a 3D structure, Figure 1. The guiding philosophy was clear: match the performance of the closed-source model, but make it faster, more efficient, and accessible to all.

### Speaking the Universal Language of Molecules
As we detailed in Part 2, the journey begins by translating a complex biological scene into a unified language the AI can understand. Boltz, like AlphaFold 3, represents the entire molecular complex---be it proteins, nucleic acids, or small-molecule ligands---as a sequence of **tokens**. Each standard building block, like an amino acid or nucleotide, is a single token. For non-standard molecules like drugs, every heavy atom becomes its own token.

This elegant solution allows the model to process vastly diverse interactions seamlessly. These tokens, initialized with sensible 3D geometries (e.g., using RDKit for ligands), are then ready to be fed into the model's “brain”: the **Pairformer trunk**.

### The Pairformer Trunk: A Deeper “Brain” for Interactions
The “brain” of the model is its trunk: a deep stack of transformer blocks that iteratively refines its understanding of the system. While AlphaFold 2 had its Evoformer, AlphaFold 3 introduced the Pairformer, a trunk focused heavily on the pairwise relationships between every token.

Boltz-1 started by closely mirroring AlphaFold 3’s architecture with a **48-layer Pairformer**. This network’s primary job is to build a rich, two-dimensional map---a **pair representation**---that encodes the geometric relationships and co-evolutionary signals between every pair of tokens.

With Boltz-2, however, the team aimed for a deeper understanding. They significantly *deepened* this cognitive engine, expanding the trunk to **64 layers**. Normally, making a transformer deeper would cause computation time to skyrocket. This is where the Boltz-2 team introduced a crucial performance optimization: the **TriFast** fused kernel.[^kernel]

The computational heart of the Pairformer lies in its special *triangular attention* operations. The TriFast kernel merges and streamlines these computations at a low level, resulting in a roughly **2x speedup** over a standard PyTorch implementation. This type of GPU-level optimization is part of a broader effort in the field to tackle the intense computational demands of geometric deep learning. In fact, NVIDIA has developed specialized libraries like cuEquivariance that provide even faster, highly-optimized CUDA kernels for these exact triangular operations, demonstrating speedups of up to 5x at the kernel level and underscoring the critical collaboration between model developers and hardware experts[[^nvidia_boltz]]. This performance gain was a game-changer for training, allowing the Boltz-2 team to use larger sub-structures (“crops”) of up to **768 tokens** per training sample. In plain terms, Boltz-2’s trunk is both larger *and* more efficient, enabling it to reason about bigger biological assemblies without slowing to a crawl.

### Evolutionary Hints: Learning from the Family Tree
AlphaFold’s secret sauce has always been its use of evolutionary insights from multiple sequence alignments (MSAs). Boltz-1 inherited AlphaFold 3's ability to *optionally* use MSAs as input, providing a valuable source of co-evolutionary information when available.

Boltz-2, however, elevates their importance, making them nearly *essential* for optimal accuracy. If you give it just a single protein sequence, the model will automatically spin up its own MSA search to gather that protein’s evolutionary cousins, ensuring valuable co-evolution signals aren't missed. In other words, Boltz-2 refuses to fly blind if there’s a family tree it can learn from.

Moreover, Boltz-2 adds native support for **paired MSAs**. This is incredibly useful when two proteins in a complex, like a receptor and its binding partner, have evolved together across many species. By telling the model which sequences belong to the same organism, it can pay special attention to inter-protein co-evolution---those subtle, correlated mutations that act as tell-tale signs of where two proteins have consistently touched throughout evolutionary history.

### Key Architectural Tweaks: From a Blueprint to a Refined Engine
Boltz-1 wasn't just a straight copy of AlphaFold 3. Its creators acted like master mechanics, analyzing the original engine to find opportunities for improvement. They introduced several clever modifications aimed at enhancing information flow, training stability, and the reliability of predictions. Many of these thoughtful changes were carried forward into Boltz-2.

#### Fixing the Information Lag
Within AlphaFold's trunk, the evolutionary data (MSA module) and the geometric blueprint (pair module) are in constant dialogue. The Boltz-1 team noticed a subtle quirk in this conversation: in each processing block, the geometric map was being updated using slightly “stale” information from the MSA's previous state.

They realized it made more sense to use the freshest insights available. They reordered the operations so that each block now performs its updates in a more logical sequence:
1.  First, refine the understanding of the evolutionary data (the MSA representation).
2.  Then, *immediately* use that updated analysis to inform the geometric blueprint (the pair representation).

This simple change ensures the model's structural hypotheses are always based on its most current evolutionary analysis. It's like ensuring a detective always works with the latest clue, making the entire learning process more direct and logically consistent.

#### Building a More Stable Sculptor
AlphaFold 3’s generative core---the diffusion model that sculpts 3D coordinates from noise---is itself a specialized transformer. Deep transformers can be notoriously difficult to train; the learning signal (gradient) can fade as it travels back through many layers, a problem known as the “vanishing gradient.”

The Boltz-1 team made a critical and elegant change to fortify this sculptor: they added standard **residual connections** to its layers. Residual connections act like informational highways, allowing the input of a layer to skip past the main processing block and be added directly to the output. This guarantees that a smooth, uninterrupted gradient can flow back to the earliest layers, even in a very deep network.

By adding these connections, Boltz-1 dramatically improved the training stability of the diffusion model. It ensured that even the initial, chaotic steps of the denoising process received a strong learning signal, reducing the chances of the generative model getting “stuck” or failing to learn effectively.

#### The Rise and Fall of the “Mega-Judge”
A great model doesn't just give answers; it knows when to be confident and when to be cautious. AlphaFold models predict their own accuracy using metrics like pLDDT and PAE. In AlphaFold 3, this was handled by a relatively small helper model.

The Boltz-1 team decided to transform this helper into an expert critic. They built a powerful, context-aware **“mega-judge”**---a 48-layer transformer as large as the main trunk itself. This judge was given a unique perspective: instead of just seeing the final predicted structure, it was shown a summary of the *entire diffusion process*. It got to watch the “movie” of the structure folding, from a random cloud of noise into its final form.

This **trajectory awareness** gave it remarkable nuance. It could spot subtle problems a normal judge might miss, like a flexible loop that was thrashing about unstably during generation. The result was a quality predictor more akin to an expert human who assesses not just the final answer, but the reasoning used to get there.

However, this powerful judge came at a steep computational cost. The Boltz team found that while it gave the best results, it significantly slowed down inference. In a pragmatic move for **Boltz-2**, they chose a more streamlined path. They “distilled” the nuanced judgment of the mega-judge into a much simpler formula: a weighted average of the original AlphaFold metrics.

Specifically, Boltz-2 reports its confidence as, on average, a mix of $0.8 \times \text{pLDDT} + 0.2 \times \text{iPTM}$. This choice sacrifices the deep reasoning of the Boltz-1 judge for speed and simplicity, making Boltz-2 a much faster and more practical tool while still providing a reliable indicator of prediction quality. It's a classic engineering trade-off: a brilliant but complex system was simplified to create a robust and widely usable one.

### Training a Titan: Jigsaws, Puzzles, and Smart Shortcuts
Training a model as massive as Boltz-1 from scratch is a monumental undertaking. The team reported a training run that took roughly four months on a cluster of high-end GPUs, a process that required not just raw compute power but also a suite of clever strategies to make the most of the available data and memory.

The biggest challenge was the sheer size of protein complexes. Trying to load an entire antibody-virus interaction into GPU memory at once is simply impossible. To solve this, the team developed a key strategy they called **crop-and-recycle**.

Think of it like learning to solve a giant jigsaw puzzle. Instead of looking at all 10,000 pieces at once, you might practice on smaller, random sections of 100 pieces. The Boltz-1 training process did something similar: for each training example, it would randomly “crop out” a manageable subset of the full complex---say, one protein domain and the part of the ligand it touches. The model would make a prediction for this small piece, and then, in the same training step, it could **recycle** its own prediction as a new input for another round of refinement. By seeing thousands of these overlapping cropped sections, the model eventually learns the rules for the entire picture.

To make the training even more effective, the team developed a **unified cropping** algorithm that could intelligently switch between two ways of picking these puzzle pieces:
* **Spatial Cropping:** This method selects all the atoms within a certain 3D radius, which is perfect for teaching the model the intricate details of a binding pocket.
* **Sequence Cropping:** This method selects a continuous piece of the 1D protein chain, which is crucial for learning about connected structural elements like helices and sheets.

By mixing these two approaches, the model was given a “well-rounded training diet,” sometimes zooming in on local interactions and other times looking at the broader structural context.

Finally, the team added a pragmatic feature for real-world use: **robust pocket conditioning**. Scientists often have some prior knowledge, like an experiment suggesting roughly where a drug binds to a protein. Boltz-1 allows users to specify this binding pocket, even partially. During prediction, the model then biases its attention to get that specific region right. This was a direct response to the needs of drug discovery researchers, transforming Boltz-1 from a pure prediction engine into a more interactive and immediately useful research partner.

These smart and efficient training strategies were essential for building a model that could achieve AlphaFold 3-like accuracy with more flexibility and speed. With the architecture refined and the training complete, let’s now dive into the core of how these models actually generate structures: the diffusion pipeline.

***

## The Diffusion Pipeline: Sculpting Structures with a Steady Hand
In Part 2 of this series, we saw how AlphaFold 3 swapped out its deterministic, piece-by-piece builder for a revolutionary new engine: a **diffusion-based generative model**. Boltz-1 and Boltz-2 embrace this same philosophy at their core, learning to sculpt a final, coherent structure out of a random, meaningless cloud of atoms.

Each of these “sculpting” steps is a sophisticated prediction. But the specific architectural choice for this generative model presented both a clever shortcut and a hidden trap.

### The Engineer's Dilemma: A Blessing and a Curse
One key design choice in both AlphaFold 3 and Boltz is that the diffusion model is **not** explicitly “equivariant.” In plain terms, the model isn't hard-coded to know that rotating a molecule in space doesn't change its internal structure. Instead, it must *learn* this concept by being shown thousands of examples in random orientations.

* **The Blessing 🙏:** This simplifies the network's architecture. Using a standard, vanilla transformer is faster and easier to scale than using complex, specialized equivariant layers.
* **The Curse ☠️:** This engineering shortcut creates two subtle but serious problems: the “canonical pose loophole” and “rotational drift.”

#### Problem 1: The Canonical Pose Loophole
During training, after the model predicts a structure, an algorithm called **Kabsch alignment** rotates the prediction to best match the true answer before the error is calculated. This is meant to be fair---the model shouldn't be punished for getting the overall orientation wrong. But this creates a loophole a “lazy” network can exploit.

A **canonical pose** is a single, fixed, “one-size-fits-all” orientation that the model might learn to output for *every* molecule, regardless of the input's orientation. The training process can't easily spot this cheat, because the Kabsch alignment step will always rotate the model's lazy prediction to match the target, giving it a low error score. The model gets a good grade without ever learning the crucial skill of how to orient a structure correctly.

For a single protein chain, this might seem minor. But for a multi-protein complex, it's a fatal flaw. If the model predicts two interacting proteins but places each in its own separate canonical pose, their relative orientation is completely random. The all-important interface between them is lost, and any predictions about their interaction are rendered meaningless.

#### Problem 2: The Rotational Drift Jitter
The second problem affects even well-behaved models. The diffusion process is iterative, taking hundreds of small denoising steps. At each step, the model's prediction will inevitably have a tiny, almost unnoticeable rigid-body mismatch---a “jitter”---relative to the current noisy structure.

When you blend the prediction with the noisy cloud, this tiny rotational error introduces a small twist. Over hundreds of steps, these innocent-looking twists accumulate, causing the entire structure to drift in a slow spiral. This isn't just visually unappealing; it can cause protein chains to become tangled or atoms to clash. It's like a stop-motion animation filmed on a wobbly tripod---each individual frame's movement is tiny, but when you play the movie, the character has drifted clear across the screen.

### Boltz to the Rescue: Align-as-You-Go and Smarter Training
The Boltz team introduced two key improvements to solve these problems, transforming the diffusion pipeline into a more stable and predictable sculptor.

#### Solution 1: Align-as-You-Go Keeps the Chisel Steady
To prevent both the canonical pose cheat and the chaotic “rotational drift,” Boltz introduces **Dynamic Kabsch Alignment at Every Step**. In each reverse diffusion step, before the model's prediction is blended with the current noisy structure, it is first perfectly aligned to it. This simple-sounding fix has profound consequences:
* **It closes the loophole:** By forcing the model to align to the correct frame at every single step, the “one-size-fits-all” canonical pose strategy becomes useless. The model has no choice but to learn the actual skill of orienting the structure correctly.
* **It cancels out drift:** The per-step alignment acts like tightening the wobbly tripod before every shot. It “re-zeros” the frame at each step, canceling out the small jitters before they can accumulate, ensuring the generation path stays smooth and direct.

![Visualizing one reverse diffusion step with and without alignment.](/images/Boltz_AF_reverse_diff.png)
Figure 2. Single reverse-diffusion step visualised with and without Align-as-you-go. The model begins from a noisy structure (open circles) and predicts a denoised pose (stars). In vanilla reverse diffusion (centre), direct interpolation yields long update vectors that twist atoms. Boltz’s variant (right) first aligns the prediction to the input, then interpolates, producing short, collinear moves and gentler geometry updates (see “Diffusion Pipeline” section).

#### Solution 2: EDM Loss Weighting Trains a Versatile Artist
The team also borrowed a trick from cutting-edge image diffusion models. Instead of treating all stages of the generation process equally during training, they applied a clever **EDM Loss Weighting** strategy. This ensures the model becomes proficient across the entire spectrum of noise levels.
* **High Noise (Early Stages):** The training loss prioritizes making bold, imaginative leaps to establish the basic global shape.
* **Low Noise (Late Stages):** The loss prioritizes making fine, precise adjustments to perfect the local geometry and polish the final result.

The upshot is a more balanced and robust learning process, yielding a model that is adept at both creating a structure from scratch and refining near-perfect details.

Together, these improvements made the Boltz diffusion pipeline remarkably stable and predictable. With this solid foundation, the stage was set for Boltz-2 to tackle one of the biggest remaining criticisms of AI-generated structures: their physical realism.

***

## From Plausible to Physically Realistic: Boltz-2 Learns the Rules of Chemistry
One common critique of deep learning structure predictors is that they can sometimes produce models that look correct at a glance but have subtle chemical implausibilities. AlphaFold 3, for all its impressive accuracy, was not immune to this. For example, researchers found it could predict the wrong “handedness” (chirality) of drug molecules, or even get the stereochemistry of unnatural D-amino acids wrong over 50% of the time[[^AF3_dpep]]---no better than a coin flip. Other issues included occasional steric clashes (atoms overlapping), unrealistic bond geometries, and warped planar groups.

Previous models tried to solve these issues with “after-the-fact” fixes, like using a penalty score to rank flawed models lower or running an energy minimization step to relax the structure. These were effective, but they felt like applying a band-aid rather than curing the underlying disease.

Boltz-2 addresses the problem at its core. It moves beyond structures that are merely *plausible* to ones that are *physically realistic* by integrating the rules of chemistry directly *during* the generation process. It introduces a system of **Boltz-Steering potentials** that act like a gentle hand on the tiller of the diffusion model, nudging it away from unphysical waters before it can get lost.

### Boltz-Steering: Blending AI with Physics
When you enable Boltz-2’s physics-guided mode, the model activates a set of built-in potential functions that act as a “chemistry conscience.” These functions, inspired by classical molecular mechanics, are designed to gently guide the AI's predictions toward physically sound conformations.

**The steering happens dynamically, at each step of the reverse diffusion process. Here’s how it works: at a given denoising step, the main AI model first makes its prediction for where the atoms should move. Simultaneously, the Boltz-Steering module evaluates the current structure against its chemistry rules and calculates a small correction. Think of it as a “guiding gradient” that points away from physical violations—for instance, away from a steric clash or toward a more ideal bond angle. This guiding gradient is then added to the AI's own update, creating a final, combined move that is informed by both the network's learned knowledge and fundamental physics.**

Importantly, the strength of this physical guidance is applied in a *time-dependent* manner. **The correction is almost zero at the beginning of the process when the structure is just a random cloud of noise—it doesn't make sense to enforce chemistry on gibberish. As the structure takes shape and the noise recedes, the influence of the steering potentials ramps up, becoming strongest near the end of the generation process when the fine details like exact bond lengths and non-clashing side-chains matter most.** This ensures the AI has freedom to make bold moves early on, while receiving precise, physics-aware guidance during the final polishing.

The key steering potentials teach the model a few core lessons:
* **Steric Clash Avoidance:** A gentle repulsive force, like an invisible spring, pushes any two non-bonded atoms apart if they get too close. This is crucial for cleaning up the crowded interfaces where molecules touch.
* **Chain Entanglement Prevention:** A mild “phantom force field” prevents separate protein chains from erroneously passing through each other, avoiding the dreaded “chain-mail jumble” that unconstrained models can sometimes produce.
* **Bond Geometry Restraints:** A set of harmonic (spring-like) potentials ensures that bond lengths and angles stay near their ideal, textbook values, preventing noticeable geometric distortions.
* **Chirality and Planarity Enforcement:** Perhaps most innovatively, Boltz-2 includes terms to preserve correct “handedness” and flatness. For each chiral center, a hefty penalty is applied if it tries to flip to its mirror image. For planar groups like aromatic rings, a flatness constraint counteracts any warping. These measures act as a crucial safety net, virtually eliminating the chirality errors seen in previous models.

It’s important to note that these potentials are applied *softly*. They guide, rather than dictate, allowing the model's learned knowledge and the physics-based forces to find a harmonious balance.

> **Wonder Box: Chemistry's Non-Negotiables**
>
> For readers less versed in chemistry, here’s a quick intuition pump for the rules Boltz-2 enforces:
> * **Chirality (Handedness):** Just like your left and right hands are non-superimposable mirror images, so are many molecules. Boltz-2 makes sure to keep the “handedness” correct---imagine a LEGO model where all pieces must fit in a certain orientation.
> * **Planarity:** Some molecular pieces are as flat as a playing card (like benzene rings). Boltz-2’s planarity check is like making sure all the “cards” in the molecular structure stay perfectly flat.
> * **Steric Clashes:** Two objects can’t occupy the same space. The model now has a built-in “personal space” rule for every atom.
> * **Chain Entanglement:** Think of two strands of cooked spaghetti. They can slide past each other, but they aren't supposed to pass through each other's solid matter. Boltz-2 adds a force that prevents this from happening.
>
> In short, Boltz-2 is teaching the AI some basic manners of chemistry: keep your hands correct, your rings flat, and don’t step on each other’s toes.

### New Levels of User Control: From AI Oracle to Interactive Partner
Beyond making the outputs more realistic, Boltz-2 also gives scientists more *control* over the modeling process, transforming it from a black-box oracle into an interactive research partner. Users can now easily inject their own expert knowledge into the prediction.

Key new control features include:
* **Method Conditioning:** A simple but clever feature where you can tell Boltz-2 what experimental context to assume with a flag (e.g., `--method x-ray` or `--method md`). This is like telling the model, “imagine this protein in a crystal lattice,” where it will favor a single, well-packed state, versus “imagine it flopping around in solution,” where it will allow for more flexibility. The full list of supported methods is extensive, including contexts like cryo-EM, solution nmr, and even theoretical model to handle various experimental and computational inputs[[^rowan_faq]]
* **Template Guidance:** Boltz-2 supercharges the use of templates (known structures of related proteins). A scientist can provide a known protein structure as a template and ask the model to perform **guided docking**---folding and docking a new ligand while keeping the protein's core structure tethered to the known conformation.
* **Custom Distance Restraints:** For advanced users, Boltz-2 accepts a simple input file with custom constraints. You can specify that two residues should be close based on experimental data, or that a disulfide bond must form. Each of these is translated into a gentle steering potential, allowing for a true human-AI collaboration in building complex structural models.

All these improvements---better architecture, stable diffusion, physical realism, and user control---make Boltz-2 a powerful platform for molecular design. But we haven’t yet touched on its headline feature: predicting the holy grail of binding affinity.

***

## The Next Frontier: Predicting the Holy Grail of Binding Affinity
Perhaps the most revolutionary aspect of Boltz-2 is that it doesn’t stop at asking, “here is how these molecules might look when bound.” It goes further to ask the most critical question in drug discovery: “*how strongly will they bind?*”

This strength of interaction, or **binding affinity**, is what separates a potential blockbuster drug from a chemical dud. For decades, accurately predicting it has been the field's holy grail. The gold-standard physics-based methods, like alchemical free-energy perturbation (FEP), are highly accurate but excruciatingly slow, taking days of supercomputer time for a single compound. This has limited their use to only a handful of the most promising, late-stage candidates.

Boltz-2’s **affinity module** shatters this paradigm. It aims to bring near-FEP accuracy to the earliest stages of discovery, providing rapid *in silico* affinity estimates in mere seconds. Let’s break down how this remarkable capability was engineered.

### How the Affinity Module Works: The Inspector
After the diffusion model generates a plausible 3D structure for the complex, that structure is handed off to a specialized “inspector”---a separate computational head fine-tuned for a single purpose: evaluating interactions.

This **affinity head** is essentially a refined Pairformer that zooms in on the binding site with a magnifying glass. It takes the rich representations from the main trunk and scrutinizes the local region, judging the quality of the fit by analyzing every atomic contact, hydrogen bond, and hydrophobic patch.

This inspector then delivers a two-part verdict:
* **A binary binding probability** (`affinity_probability_binary`): A simple 0-to-1 score answering, “Is this molecule likely to be a true binder, or is it a dud?” This is incredibly useful for **virtual screening**, allowing researchers to sift through hundreds of thousands of compounds to find the few that are actually worth pursuing.
* **A predicted affinity value** (`affinity_pred_value`): A numerical estimate of the binding strength, given as pIC<sub>50</sub>. This is the key metric for **lead optimization**, where chemists need to know if changing a molecule's structure will make it bind ten times more tightly.

### Training the Inspector: A Diet of Data and Deception
To build this inspector, the Boltz team needed to teach it the subtle language of physical chemistry. This required a monumental data engineering effort and several clever training strategies.

First, they compiled a massive dataset of over a **million experimental binding measurements** from public databases like ChEMBL and BindingDB. This included both quantitative values (like K<sub>d</sub> and IC<sub>50</sub>) and qualitative, yes/no results from high-throughput screens.

Second, to ensure the model learned the right lessons, they used a crucial trick: **synthetic decoys**. For each known active drug, they found a similarly-sized molecule with a different shape and chemistry that was known to be inactive. By training on these look-alike pairs, the model was forced to learn the difference between true *interaction patterns* and superficial ligand properties.

Finally, they pushed the model further using **distillation** from its predecessor. Boltz-1 was used to generate thousands of extra training examples of plausible (but not always perfect) binding poses. Boltz-2’s affinity head was then trained to predict Boltz-1’s “opinion” on these structures, exposing it to a wider variety of poses and negative examples than static experimental data alone could provide.

### The Payoff: Real-World Impact and Future Promise
The result of this intensive training is a model with a striking capability. On standard industry benchmarks, Boltz-2’s affinity predictions achieve a Pearson correlation of around 0.60--0.66 with experimental results. To put that in context, it's in the same ballpark as some rigorous physics-based methods and far surpasses traditional docking scores. While not perfect, it represents a huge leap forward for AI in understanding the subtle energetics of binding.

It’s important to note the current caveats: the model only considers direct protein-ligand interactions (no explicit water molecules) and is not yet suited for protein-protein binding affinities. But as a proof of concept, Boltz-2 shows that a single neural network can master both structure and property prediction. It hints at a future where an AI can not just model a proposed drug, but also foretell its other properties, like toxicity or metabolism, all in one go.

***

## Conclusion and Outlook: The Open Road Ahead
The journey from AlphaFold 2 to Boltz-2 is more than a story of technical achievement; it exemplifies the incredible velocity of a scientific field when a breakthrough is made open and accessible. In Part 1 and Part 2, we witnessed closed-source models redefine the limits of structural prediction. Now, in Part 3, we've seen an open-source community not only match those achievements but push beyond them, integrating the features that biologists and chemists care about most: physical realism, user control, and ultimately, function.

Boltz-1's release under a permissive MIT license was the spark. It invited the entire world to tinker, improve, and build upon its foundation, leading to rapid adoption and innovation. Boltz-2 fanned that spark into a flame, transforming the field's goal from pure *structure prediction* to the frontier of *function prediction*. We can now ask an AI, “if I stick this molecule onto that protein, how tight will it stick?”—a question whose answer was, just a few years ago, the exclusive domain of slow and costly physics simulations.

This leap points toward an exciting future. The integration of explicit physics via steering potentials shows that we don't have to choose between a “black-box” AI and a classical simulator; we can have hybrids that leverage the pattern-recognition strengths of one and the ground-truth principles of the other. One can easily envision a future model that incorporates a fast molecular mechanics engine directly in the loop, or one that predicts not just static affinity but the kinetics of binding—the on and off rates that truly govern a drug's behavior in the body.

Furthermore, the Boltz-2 model represents a new paradigm: the **biomolecular foundation model**. Much like large language models provide a foundation for countless NLP tasks, Boltz-2—trained on an immense swath of sequence, structure, and binding data—can be fine-tuned and adapted for a huge range of applications, from enzyme engineering to automated drug design. Its open nature means the community can build specialized plug-ins to handle cofactors or metal ions, or create entirely new offshoots, like **Chai-1** model [[^chai1]] for antibody design.

In closing, the evolution from AlphaFold 2 to Boltz-2 is a case study in the virtuous cycle of scientific progress. A breakthrough (AlphaFold) was made accessible (OpenFold, then Boltz), which in turn enabled new breakthroughs (Boltz-2’s affinity prediction). Each generation builds upon the last, and more people can participate because the tools are open to all. The result is not just incremental improvement, but a qualitative shift in what's possible. We are moving from solving static structures to co-designing dynamic interactions, and doing it in a way that empowers everyone to contribute. That bodes well for the future of molecular science, where collaboration—between humans, and now with our AI partners—will unlock a deeper understanding of biology and faster innovation in treating disease.



***

[^sengarAF2blog]: Sengar, A. Deconstructing the Fold with AlphaFold 2. *Blog Post*, 2025. [link](https://adityasengar.github.io/alphafold2/)
[^sengarAF3blog]: Sengar, A. Denoising the Complex with AlphaFold 3. *Blog Post*, 2025. [link](https://adityasengar.github.io/alphafold3/)

[^boltz1]: Wohlwend, J., Corso, G., *et al*. (2024). *Boltz-1: Democratizing Biomolecular Interaction Modeling*. bioRxiv preprint **2024.11.19.624167** [link](https://www.biorxiv.org/content/10.1101/2024.11.19.624167v1)
[^boltz2]: Passaro, S., Corso, G., Wohlwend, J., *et al*. (2025). *Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction*. bioRxiv preprint **2025.06.14.659707**. [link](https://www.biorxiv.org/content/10.1101/2025.06.14.659707v1)
[^bioit_boltz2]: Bio-IT World (2025, June 6). *MIT Researchers Unveil Boltz-2: AI Model Predicts Protein Structure, Binding Affinity in Seconds*. [link](https://www.bio-itworld.com/news/2025/06/06/mit-researchers-unveil-boltz-2-ai-model-predicts-protein-structure-binding-affinity-in-seconds)
[^fep]: FEP is a cornerstone of computational chemistry, providing highly accurate predictions of binding affinity by rigorously simulating the physical transformation of one molecule into another. This accuracy, however, comes at an immense computational cost, which has historically limited its use to only a handful of the most promising drug candidates.
[^kernel]: A “fused kernel” is a GPU programming optimization that combines multiple, separate computational steps into a single, highly-efficient operation (a “kernel”). Instead of reading data from memory, performing one calculation, writing the result back, and then repeating the process for the next step, a fused kernel performs the entire sequence of operations in on-chip memory (registers or shared memory) without intermediate writes to slower global memory. This dramatically reduces memory latency and maximizes computational throughput. The **TriFast** kernel, developed with NVIDIA, specifically fuses the complex triangular attention updates, which are a major bottleneck, leading to significant speedups.
[^rowan_faq]:  *The Boltz-2 FAQ*. [link](https://rowansci.com/blog/boltz2-faq)
[^nvidia_boltz]: NVIDIA Developer Blog. *Accelerated Molecular Modeling with NVIDIA cuEquivariance
and NVIDIA NIM microservices*. [link](https://developer.nvidia.com/blog/accelerated-molecular-modeling-with-nvidia-cuequivariance-and-nvidia-nim-microservices/)
[^AF3_dpep]: Childs, H., Zhou, P., & Donald, B. R. (2025). Has AlphaFold 3 Solved the Protein Folding Problem for D-Peptides?. bioRxiv, 2025-03. [link](https://www.biorxiv.org/content/10.1101/2025.03.14.643307v1.abstract)
[^chai1]: Chai Discovery team, Boitreaud, J., Dent, J., McPartlon, M., Meier, J., Reis, V., ... & Wu, K. (2024). Chai-1: Decoding the molecular interactions of life. BioRxiv, 2024-10. [link](https://www.biorxiv.org/content/10.1101/2024.10.10.615955v2.abstract)
[^trifast]: Liam Atkinson.Trifast. github [link](https://github.com/latkins)
