---
layout: post
title: "DDPM"
hidden: true
date: 2025-07-02
---
# From Noise to Art: A Deep Dive into Diffusion Models and the EDM Revolution

*July 2, 2025*

If you've been anywhere near the AI space recently, you've seen the stunning images produced by models like DALL-E 3, Midjourney, and Stable Diffusion. The technology behind many of these state-of-the-art generators is a class of models known as **Diffusion Models**.

At first glance, their inner workings seem almost magical. They start with pure random noise and meticulously sculpt it into a coherent, often beautiful, image. But how? This post will break down the core concepts behind diffusion models, starting with the original "old school" methods like DDPM and moving to the game-changing insights from the landmark "Elucidating the Design Space of Diffusion-Based Generative Models" (EDM) paper that redefined the state of the art.

---

## The Core Idea: A Story of Corruption and Redemption üñºÔ∏è

Imagine taking a masterpiece painting and gradually adding layers of digital noise until it's an unrecognizable static mess. Now, what if you could train an AI to perfectly reverse that process? What if it could learn, step-by-step, how to remove the noise to restore the original painting?

If your AI gets good enough at this "denoising" task, it could theoretically start with a *brand new* canvas of random noise and "denoise" it into a completely novel masterpiece that has never existed before.

This is the central idea of diffusion models. It's a two-part story:

1.  **Forward Diffusion:** The process of methodically and slowly destroying an image by adding Gaussian noise.
2.  **Reverse Diffusion:** The process of training a neural network to undo the noise, step-by-step, to generate an image.

Let's look at each part in more detail, starting with how things were originally done.

---

### The "Old School" Way: A Look at DDPM

The foundational 2020 paper, "Denoising Diffusion Probabilistic Models" (DDPM), laid out the blueprint that inspired much of what followed. Their approach was characterized by a few key design choices:

* **Forward Process:** They used a **Variance Preserving (VP)** approach. At each of the `T=1000` discrete timesteps, they added a small amount of noise according to a **linear noise schedule**. This means the amount of noise added increased linearly from a tiny value (`Œ≤‚ÇÅ`) to a slightly larger one (`Œ≤_T`).
* **Reverse Process:** They trained a U-Net model to predict the noise (`œµ`) that was added to the image at a given timestep `t`. The loss function was a simple, unweighted Mean Squared Error (MSE) between the real noise and the predicted noise.
* **Sampling:** To generate an image, they started with pure noise and iterated backwards for all 1000 steps. Each step involved using the model's prediction and then adding a small amount of random noise back in. This made the process **stochastic** and very, very slow.

While groundbreaking, this original formula was rigid and computationally expensive. It worked, but it left a lot of room for improvement.

---
## The Modern Approach: The EDM Revolution üöÄ

The 2022 NVIDIA paper, "Elucidating the Design Space of Diffusion-Based Generative Models" (EDM), blew the doors open by systematically re-evaluating every part of the process. They argued that previous approaches were "unnecessarily convoluted" and presented a simplified, unified framework that supercharged performance.

Here‚Äôs how they changed the game, moving from the old way to the new.

### 1. From Fixed Schedules to a Continuous Design Space

**The Old Way (DDPM):** Used a fixed, discrete `T=1000` step linear schedule for `Œ≤t`. This was simple but not necessarily optimal. The noise increase was uniform, which didn't reflect the fact that removing the last bits of noise is a more delicate operation than clearing out the initial chaos.

**The EDM Way:** Threw out discrete timesteps `t` in favor of a continuous noise level `œÉ`. This is a more flexible and powerful abstraction. Now, instead of a fixed schedule, you can design a continuous path from maximum noise `œÉ_max` to zero noise `œÉ_min`. This allowed them to introduce a new schedule, parameterized by `œÅ` (rho), that spends more steps at lower noise levels, preserving fine details where it matters most.

**What it means:** You get higher-quality images because the model's effort is better distributed. You're no longer locked into a single, suboptimal schedule.

### 2. From Simple Prediction to Principled Preconditioning

**The Old Way (DDPM):** The U-Net was trained to predict `œµ`. The inputs and outputs of the network weren't specially scaled, meaning the network had to learn to handle vastly different signal-to-noise ratios internally.

**The EDM Way:** This is their most critical insight. They introduced **network preconditioning**. They wrapped the U-Net `F_Œ∏` in scalers that are functions of the noise level `œÉ`:

`D(x; œÉ) = c_skip(œÉ) * x + c_out(œÉ) * F_Œ∏(c_in(œÉ) * x; œÉ)`

* `c_in(œÉ)` scales the input, ensuring the network always sees data with a consistent variance (e.g., variance of 1).
* `c_out(œÉ)` scales the output, ensuring the network's prediction is correctly proportioned for the given noise level.
* `c_skip(œÉ)` provides a direct skip connection from the input `x`, which helps the model make small, precise adjustments.

**What it means:** The U-Net's job becomes much easier. It can focus on learning the core denoising patterns because it's no longer fighting a wildly changing scale of inputs and outputs. This leads to much faster and more stable training and ultimately, a more powerful denoiser.

### 3. From a Slow Crawl to a Fast Leap: Advanced Samplers

**The Old Way (DDPM):** Used a slow, stochastic sampler that took 1000 steps. Each step was like a 1st-order Euler step‚Äîa simple, but not very accurate, way to follow the denoising path.

**The EDM Way:** They showed that sampling is just solving an Ordinary Differential Equation (ODE). By applying more advanced numerical solvers, you can take much larger, more accurate steps. They popularized the **2nd-order Heun solver**, which makes a prediction and then uses that prediction to correct its path, all within a single step.

**What it means:** A massive speed-up. Instead of 1000 steps, you can now get state-of-the-art images in as few as **35-80 steps**. The generation process becomes deterministic (no extra noise added), meaning the same initial noise will always produce the same image, which is great for reproducibility.

---

## A Practical Guide: Choosing Your Parameters in the EDM Era

The EDM framework gives you more control, but with great power comes the question: how do you choose the parameters? Here‚Äôs a quick guide.

* **The Solver:** For most use cases, start with the **deterministic Heun solver**. It offers the best balance of speed, quality, and reproducibility. You might consider a stochastic sampler (one that adds noise back in) only if you need to explore more diverse outputs from a single starting noise or if the deterministic path seems to be producing artifacts.

* **Number of Steps (`N`):** This is a direct trade-off between quality and speed.
    * **For fast previews:** Start with `N = 20-25`.
    * **For high quality:** A value of `N = 40-80` is often enough to achieve results indistinguishable from models using 1000+ steps.

* **The Timestep Schedule (`œÅ`):** This parameter controls how the `N` steps are distributed across the noise levels.
    * **Recommended value:** `œÅ = 7` (from the paper) is an excellent default. It concentrates steps at low `œÉ` values, which is crucial for generating fine details. Higher values of `œÅ` will concentrate the steps even more at the end of the process.

* **Stochasticity (`S_churn`, `S_noise`):** These parameters control how much randomness is added back during sampling, turning a deterministic solver into a stochastic one.
    * **Recommendation:** Start with `S_churn = 0` (fully deterministic). Only introduce a small amount of churn if you feel the model is getting stuck or you want to add variety. This is an advanced setting for fine-tuning.

By understanding these principles, you can see how diffusion models evolved from a fascinating academic curiosity into the robust, efficient, and high-quality image generation powerhouses they are today.

***

This video provides a great high-level intuition for how diffusion models work by visualizing the score-matching perspective, which underpins the entire process.

[![YouTube video on Diffusion Models](https://img.youtube.com/vi/B4oHJpEJpEJpBAA/0.jpg)](https://www.youtube.com/watch?v=B4oHJpEJBAA)

The video linked here offers a clear explanation of diffusion models from a score-based perspective, which helps connect the concepts of DDPM and the more generalized EDM framework.
http://googleusercontent.com/youtube_content/1

*Written on July 2, 2025*
