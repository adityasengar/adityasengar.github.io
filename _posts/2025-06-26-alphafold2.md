---
layout: post
title: "AlphaFold2"
date: 2025-06-26 14:30:00 +0200  
---
{% raw %}

# From AlphaFold2 to Boltz-2: A Journey Through the Revolution of Protein Structure Prediction

*By Aditya Sengar*
*June 26, 2025*

***

### Introduction: A New Era in Structural Biology

The last few years have seen a revolution in structural biology, driven by the breakneck speed of advances in artificial intelligence. What was once the domain of decades-long experimental work is now being reshaped by models that can predict the atomic structure of a protein in minutes.

This post is the first in a series where we will deconstruct the key architectures that have defined this new era, from the deterministic brilliance of AlphaFold 2 to the generative power of diffusion models like Boltz-2. To understand where the field is going, we must first build a deep appreciation for the foundation upon which it all stands.

**In this first article, we will focus entirely on AlphaFold 2**, the model that started the revolution. We will move beyond a high-level overview to dissect its two-part engine: the **Evoformer** trunk, which established a rich, *bidirectional dialogue* between evolutionary data and a geometric hypothesis, and the deterministic **Structure Module**, which used the intricate, purpose-built Invariant Point Attention (IPA) to translate this blueprint into a precise 3D structure. By the end, you will have a solid, technically-grounded understanding of how AlphaFold 2 works, setting the stage for our exploration of the models that followed.

## AlphaFold 2’s Evoformer & Structure Module

Long before diffusion models entered the mainstream for structural biology, AlphaFold 2 (Jumper et al., 2021) achieved a watershed moment. When DeepMind presented their results at the CASP14 competition in late 2020, they didn't just improve upon existing methods; they shattered all expectations, leaving the entire field of computational biology wondering how it was possible (Service, 2020).

When the paper and code were finally released (eight months later), the "secret sauce" was revealed. It wasn't a single, magical insight into protein folding, but rather a masterpiece of deep learning engineering that fundamentally reinvented how to reason about protein structure. (For another excellent high-level overview, see the post by Outeiral, 2021). The computational heart of this masterpiece consists of two main engines working in concert: the **Evoformer** and the **Structure Module**.

<br>

![High-level overview of the AlphaFold2 architecture. The model is conceptually divided into three stages. First, input features are generated from the primary sequence, a Multiple Sequence Alignment (MSA), and optional structural templates. Second, the Evoformer block iteratively refines an MSA representation and a pair representation in a deep communication network. Finally, the Structure Module translates the refined representations into the final 3D atomic coordinates.](/images/AF2.png)
*High-level overview of the AlphaFold2 architecture.*
<br>

> ### Key Concepts Explained
> - **RMSD (Root Mean Square Deviation)**: This is the classic way to measure the similarity between two 3D structures. You optimally superimpose the predicted structure onto the true one and then calculate the average distance between their corresponding atoms. A lower RMSD means a better prediction.
>
> - **FAPE (Frame Aligned Point Error)**: This is AlphaFold 2's primary loss function and a clever improvement over RMSD. Instead of one global superposition, FAPE measures the error of all atoms from the perspective of **every single residue's local frame**. This means it heavily penalizes local errors—like incorrect bond angles or side-chain orientations—that a global RMSD might miss. It effectively asks for every residue: "From my point of view, are all the other atoms where they should be?"
>
> - **Distogram (Distance Histogram)**: A 2D plot where the pixel at position $(i, j)$ doesn't just show a single predicted distance, but a full probability distribution across a set of distance "bins." For example, it might predict a 70% chance the distance between residue $i$ and $j$ is 8-10 Å, and a 30% chance it's 10-12 Å.
>
> - **BERT (Bidirectional Encoder Representations from Transformers)**: A technique from natural language processing where a model is trained by randomly hiding words in a sentence and trying to predict them from context. AlphaFold 2 applies this to the "language" of evolution by masking amino acids in the MSA and forcing the model to predict them, strengthening its understanding of evolutionary patterns.

### The Evoformer's Engine: The Attention Mechanism

The Evoformer's architecture is built on the now-ubiquitous **transformer** (Vaswani et al., 2017). Its core computational tool is the **attention mechanism**, a powerful technique that allows the model to learn context by weighing the influence of different parts of the input data on each other. When processing a specific residue, for example, it learns how much "attention" to pay to every other residue in the protein, allowing it to dynamically identify the most important relationships. For an excellent visual explanation of this concept, readers are encouraged to see the work by Alammar (2018).

This allows the model to reason about which pieces of information—both in the evolutionary alignment and in the geometric blueprint—are most relevant for making its next prediction. A detailed, step-by-step numerical example of how self-attention works is provided in the Appendix for interested readers.

### Building the Blueprint: The Representations and Templates
The self-attention mechanism is a powerful tool, but for it to work on a problem as complex as protein structure, it needs data representations that are far richer than simple word embeddings. For years, the state-of-the-art approach was to take a Multiple Sequence Alignment (MSA) and distill its evolutionary information down into a single, static 2D contact map. Methods would analyze the MSA to predict which pairs of residues were likely touching, and this map of contacts would then be fed as restraints to a separate structure-folding algorithm (Ovchinnikov, 2018).

AlphaFold 2’s paradigm shift was to **avoid this premature distillation**. Instead of collapsing the rich evolutionary data into a simple 2D map, it builds and refines two powerful representations in parallel, providing the perfect canvas for its attention-based Evoformer. Let’s look at how these representations are constructed.

#### The MSA Representation: A Detailed Evolutionary Profile
This is far more than just the raw alignment of sequences. AlphaFold 2 first clusters the MSA. The model then processes a representation of size ($N_{\text{seq}} \times N_{\text{res}}$), where $N_{\text{seq}}$ is the total number of sequences in the stack (composed of both MSA cluster representatives and structural templates) and $N_{\text{res}}$ is the number of residues.

The key is that for each residue in a representative sequence, the model starts with a rich, 49-dimensional input feature vector. To make this concrete, imagine the model is looking at residue 50 of a representative sequence, which is a Leucine (L). Its input vector would answer several questions simultaneously:
* **What is this residue?** A set of channels acts as a one-hot encoding, where the channel for "Leucine" is set to 1 and all others are 0.
* **What is happening around it?** Other channels are dedicated to deletion information, indicating if a deletion exists and tracking the average number of deletions seen in that cluster at that position.
* **What is its evolutionary context?** A large portion of the vector is dedicated to the cluster's statistical profile. This profile summarizes all the sequences in that family branch, saying something like: *"For position 50, Leucine appears 70% of the time, Isoleucine appears 20%, and Valine appears 5%."*

This 49-dimensional input vector is then processed by a linear layer to produce the internal MSA representation, which has 256 channels ($c_m=256$). This is the representation that is iteratively refined within the Evoformer.

#### The Pair Representation: The Geometric Blueprint
This is a square matrix of size ($N_{\text{res}} \times N_{\text{res}}$) with 128 channels ($c_z=128$) that evolves into a detailed geometric blueprint of the protein. It's the model's internal hypothesis about the 3D structure. It doesn't start as a blank slate; the initial representation $\mathbf{z}_{ij}$ is constructed from the primary sequence itself, seeding the model with fundamental information about residue identity and position. This is done in two steps:

##### 1. Encoding Residue Pairs via Outer Sum.
First, the model learns two separate 128-dimensional vector representations, $\mathbf{a}_i$ and $\mathbf{b}_j$, for each amino acid at positions $i$ and $j$. These are learned by passing the one-hot encoding of the amino acid type through two different linear layers. The initial pair information is then created by simply adding these vectors:

$$\mathbf{z}_{ij}^{\text{pairs}} = \mathbf{a}_i + \mathbf{b}_j$$

This operation creates a basic ($N_{\text{res}} \times N_{\text{res}}$) map where each entry is a 128-dimensional vector influenced by the specific amino acids at positions $i$ and $j$.

##### 2. Encoding Relative Position.
Next, the model explicitly injects information about the 1D sequence separation. It calculates the relative distance $d_{ij} = i - j$, which is clipped to a fixed window (e.g., between -32 and +32) and converted into a one-hot vector. This binary vector is then passed through another linear layer to create a dedicated 128-dimensional positional embedding, $\mathbf{p}_{ij}$.

The final initial pair representation is the sum of these two components:

$$z_{ij}^{\text{initial}} = z_{ij}^{\text{pairs}} + p_{ij}$$

This initial grid is like digital graph paper, with basic 1D relationships sketched out. The job of the Evoformer is to enrich this grid, filling its 128 channels with sophisticated 3D information like residue-pair distances and orientations.

#### Structural Templates: A Powerful Head Start
The third, optional source of information comes from **structural templates**. If homologous structures exist in the PDB, AlphaFold 2 leverages their known geometry not as a rigid scaffold, but as a set of powerful, editable hints. This information is integrated through two distinct and sophisticated pathways:

##### 1. A 2D Geometric Hint for the Pair Representation.
First, the model extracts geometric data from each template, such as a **distogram** (a map of binned Cβ-Cβ distances) and the relative orientations of its residues. (These features are precisely defined in the supplementary information. The **template_distogram** discretizes the measured Cβ-Cβ distance for every residue pair into a probability distribution over 39 bins from 3.25 Å to over 50.75 Å. The **relative orientation** `template_unit_vector` points from one residue's frame to another's Cα atom, but the authors note this feature was set to zero in the trained models.) This `template_pair_feat` tensor is processed by its own dedicated "Template Pair Stack," which uses a simplified version of the Evoformer's triangular updates to refine the template's geometric information.

The crucial step is how this is incorporated. For each residue pair $(i, j)$ in its own `pair_representation`, the model uses an attention mechanism (`TemplatePointwiseAttention`) to look at the corresponding information from all available templates. It effectively asks: *"For this specific pair, which of my templates offers the most reliable geometric clue?"* It then takes a weighted average of the hints from all templates and adds that to its own geometric blueprint. This allows the model to intelligently fuse information, trusting one template for a local helix and another for a distant domain interaction.

##### 2. A 1D "Advisor" for the MSA Representation.
Second, in a particularly elegant move, the model extracts the known backbone torsion angles from the templates. These angles are embedded into a feature vector and then **concatenated directly to the MSA representation** as if they were additional sequences.
# MSA Representation Mathematical Description
To be precise, the main MSA representation, $$M_{\text{msa}}$$, has shape $$(N_{\text{clust}} \times N_{\text{res}} \times c_m)$$, where $$N_{\text{clust}}$$ is the number of clustered sequences and $$c_m = 256$$ is the number of channels. The template torsion angle features are first passed through a small MLP to create a template representation, $$M_{\text{templ}}$$, with a compatible shape of $$(N_{\text{templ}} \times N_{\text{res}} \times c_m)$$. The concatenation happens along the sequence dimension:

$$
M_{\text{final}} = \text{concat} \left( \left[ M_{\text{msa}}, M_{\text{templ}} \right], \, \text{axis}=0 \right)
$$

The resulting matrix, $$M_{\text{final}}$$, has shape $$((N_{\text{clust}} + N_{\text{templ}}) \times N_{\text{res}} \times c_m)$$. This larger matrix, where $$N_{\text{seq}} = N_{\text{clust}} + N_{\text{templ}}$$, is what the Evoformer processes.


This treats the template not as a static map, but as an expert participant in the evolutionary dialogue. By sitting alongside the other sequences, its structural information can directly bias the MSA attention mechanisms. For example, if a template's torsion angles clearly define a beta-strand, it can encourage the MSA attention to focus on finding the long-range co-evolutionary signals that are characteristic of beta-sheet formation.

### The Evoformer's Work Cycle: A Refinement Loop
The Evoformer's power comes from repeating a sophisticated block of operations 48 times. Each pass through this block represents one full cycle of the "dialogue" between the evolutionary and geometric representations.

The goal is to use the attention tools we've just learned about to enrich both the MSA and Pair representations, making each one more accurate based on feedback from the other. A single cycle, or block, consists of three main stages: updating the MSA stack, updating the pair stack, and facilitating their communication.

#### Stage 1: Processing the Evolutionary Data (The MSA Stack)
The block first focuses on the MSA representation to extract and refine co-evolutionary signals. This is done with a specialized MSA-specific attention mechanism.

##### Axial Attention. 
To handle the massive ($N_{seq} \times N_{res}$) MSA matrix, the model doesn't compute attention over all entries at once. Instead, it "factorizes" the attention into two much cheaper, sequential steps:
* **Row-wise Gated Self-Attention.** Performed independently for each sequence (row), this step allows the model to find relationships between residues *within* a single sequence.
* **Column-wise Gated Self-Attention.** Performed independently for each residue position (column), this step allows the model to compare the "evidence" from all the different evolutionary sequences for that specific position in the protein.

##### MSA Transition. 
After the attention steps, the MSA representation passes through a simple but important `MSATransition` layer. This is a two-layer MLP that is applied point-wise to every vector in the MSA representation, allowing for more complex features to be learned.

#### Stage 2: Enforcing Geometry (The Pair Stack)
The block now turns to the pair representation ($\mathbf{z}$), the model's geometric blueprint. At this stage, the blueprint might contain noise or local predictions that are not globally consistent. The goal of the Pair Stack is to refine this blueprint by enforcing geometric constraints across the entire structure. Its primary tool is a set of novel "triangular" operations, which are based on a simple, powerful idea: information about the relationship between residues $(i, k)$ and $(k, j)$ provides strong evidence about the "closing" edge of the triangle, the relationship $(i, j)$.

As illustrated in the figure below, the model uses two distinct mechanisms to pass information through all possible residue triplets, ensuring the final blueprint is physically plausible.

<br>

![The triangular operations at the heart of the Evoformer. (b) A triplet of residues (i, j, k) corresponds to a set of edges in the pair representation. (c) The information on edge (i,j) (highlighted in yellow) is updated by systematically incorporating information from the other two edges in the triangle. This is done sequentially via multiplicative updates and self-attention, considering both "outgoing" edges (from node i) and "incoming" edges (to node j).](/images/triangle_update.png)
*The triangular operations at the heart of the Evoformer.*
<br>

##### Triangular Multiplicative Update: Densifying the Geometric Graph.
This first operation acts as a fast, "brute-force" way to strengthen local geometric signals. It considers every possible intermediate residue $k$ for each pair $(i,j)$ and aggregates the evidence from all of them. It combines information from the other two edges of the triangle. The process is gated to control the information flow:

1. It first computes an **update vector** by combining information from the other two edges in the triangle. For the "outgoing" update, the logic is:

$$
\text{update\_vector} = \sum_{k} \left( \text{Linear}(z_{ik}) \odot \text{Linear}(z_{jk}) \right)
$$

2. It then computes a **gate vector**:

$$
g_{ij} = \text{sigmoid}(\text{Linear}(z_{ij}))
$$

which acts as a dynamic filter based on the current state of the edge being updated.

3. Finally, the gated update is applied:

$$
z_{ij} \mathrel{+}= g_{ij} \odot \text{update\_vector}
$$


By summing over all possible intermediate residues $k$, this mechanism efficiently ensures that the information in the pair representation is locally consistent and dense. The entire process is repeated symmetrically for "incoming" edges.

##### Triangular Self-Attention.
While the multiplicative update is powerful, it treats all triangles equally. **Triangular Self-Attention is the more sophisticated and arguably more critical operation**, as it allows the model to be selective and propagate information over long distances. The representation for an edge $(i, j)$ acts as a "query" to selectively gather information from other edges.

Let's break down the "starting node" version into its key steps:
1.  **Project to Query, Key, Value.** First, the model projects the pair representations into Query, Key, and Value vectors. For our query edge, we have $\mathbf{q}_{ij} = \text{Linear}(\mathbf{z}_{ij})$. For the edges it will attend to, we have $\mathbf{k}_{ik} = \text{Linear}(\mathbf{z}_{ik})$ and $\mathbf{v}_{ik} = \text{Linear}(\mathbf{z}_{ik})$.

2.  **Calculate Attention Score with Triangle Bias.** For each potential interaction between edge $(i,j)$ and edge $(i,k)$, the model calculates a score. Crucially, this score includes a learned **bias** that comes directly from the triangle's closing edge, $\mathbf{z}_{jk}$:
    $$
    \text{score}_{ijk} = \frac{\mathbf{q}_{ij} \cdot \mathbf{k}_{ik}}{\sqrt{d_k}} + \text{Linear}(\mathbf{z}_{jk})
    $$
    This allows the model to ask a sophisticated question: *"How relevant is edge $(i,k)$ to my query $(i,j)$, given my current belief about the geometry of the third edge $(j,k)$?"* This is what allows a confident local prediction in one part of the protein to rapidly inform a less confident prediction far away in the sequence.

3.  **Normalize and Gate.** The scores for a given $i,j$ are passed through a `softmax` function over all possible nodes $k$ to get the final attention weights, $\alpha_{ijk}$. Separately, a gate vector, $\mathbf{g}_{ij} = \text{sigmoid}(\text{Linear}(\mathbf{z}_{ij}))$, is computed. The final output is the gated, weighted sum of all the value vectors:
    $$
    \text{output}_{ij} = \mathbf{g}_{ij} \odot \sum_k \alpha_{ijk} \mathbf{v}_{ik}
    $$
This entire process is then repeated symmetrically for the "ending node," where edge $(i,j)$ attends to all edges ending at node $j$. The $\mathbf{O(N_{res}^3)}$ complexity of these triangular operations makes them the primary computational bottleneck in AlphaFold 2, but they are essential for creating a globally consistent geometric blueprint.

##### Pair Transition.
Finally, just like the MSA stack, the pair stack processing concludes with a `PairTransition` layer. This is a point-wise, two-layer feed-forward network (MLP) that is applied independently to each vector $\mathbf{z}_{ij}$ in the pair representation.

Its architecture follows a standard transformer design: the first linear layer expands the representation's channels by a factor of 4 (from 128 to 512), a ReLU activation is applied, and the second linear layer projects it back down to 128 channels. This expansion-and-contraction structure allows the model to perform more complex, non-linear transformations on the features for each pair, helping it to better process and integrate the rich information gathered from the preceding triangular updates and attention steps.

#### Stage 3: The Bidirectional Dialogue (The Communication Hub)

![The core logic of an AlphaFold2 Evoformer block. Information is exchanged between the 1D MSA representation and the 2D pair representation. The key innovation is the triangular self-attention mechanism within the pair representation, which enforces geometric consistency by reasoning about triplets of residues (i, j, k).](/images/Evoformer.png)
*The core logic of an AlphaFold2 Evoformer block.*
<br>

The MSA and Pair stacks don't operate in isolation. The true genius of the Evoformer is how they are forced to communicate within each block, creating a virtuous cycle where a better evolutionary model informs the geometry, and a better geometric model informs the search for evolutionary clues. This dialogue happens through two dedicated pathways.

##### Path 1: From MSA to Pairs (The Outer Product Mean).
This is the primary pathway for co-evolutionary information to update the geometric hypothesis. The mechanism, called the `OuterProductMean`, effectively converts correlations found across the MSA's sequences into updates for the pair representation.

For a given pair of residues $(i, j)$, the model takes the representation for residue $i$ and residue $j$ from *every* sequence $s$ in the MSA stack ($\mathbf{m}_{si}$ and $\mathbf{m}_{sj}$). It projects them through two different linear layers, calculates their outer product, and then averages these resulting matrices over all sequences. In pseudo-code:
$$\text{update for } \mathbf{z}_{ij} = \text{Linear} \left( \text{mean}_{s} \left( \text{Linear}_a(\mathbf{m}_{si}) \otimes \text{Linear}_b(\mathbf{m}_{sj}) \right) \right)$$
This operation is powerful because if there is a consistent pattern or correlation between the features at positions $i$ and $j$ across many sequences, it will produce a strong, non-zero signal in the averaged matrix. This directly injects the statistical evidence from the entire MSA into the geometric blueprint, telling it which residue pairs are likely interacting.

##### Path 2: From Pairs to MSA (The Attention Bias).
This is the equally important reverse pathway, where the current geometric hypothesis guides the interpretation of the MSA. This happens subtly during the **MSA row-wise attention** step.

When the model calculates the attention score between residue $i$ and residue $j$ within a single sequence, it doesn't just rely on comparing their query and key vectors. It adds a powerful bias that comes directly from the pair representation.
$$\text{score}(\mathbf{q}_{si}, \mathbf{k}_{sj}) = \frac{\mathbf{q}_{si} \cdot \mathbf{k}_{sj}}{\sqrt{d_k}} + \text{Linear}(\mathbf{z}_{ij})$$
The effect is profound. If the pair representation, $\mathbf{z}_{ij}$, already contains a strong belief that residues $i$ and $j$ are in close contact, the bias term will be large. This forces the MSA attention to focus on that pair, effectively telling the MSA module: "Pay close attention to the relationship between residues $i$ and $j$ in this sequence; I have a strong geometric reason to believe they are linked, so any co-evolutionary signal here is especially important." This allows the geometric model to guide the search for subtle evolutionary signals that confirm or refine its own hypothesis.

### The Structure Module: From Feature Maps to Atomic Coordinates
After 48 Evoformer iterations the network possesses two mature tensors: a per-residue feature vector $\mathbf{s}_i$ ("single representation") and a per-pair tensor $\mathbf{z}_{ij}$ ("pair representation"). The **Structure Module** must now turn these high-level statistics into a concrete three-dimensional model. It does so through eight rounds of a custom transformer block called *Invariant Point Attention* (IPA) (Phie, 2023) followed by a small motion model, *Backbone Update*. The entire pipeline is **differentiable**.

> ### What Does "End-to-End Differentiable" Mean?
> The term **end-to-end differentiable** describes the property that allows a deep learning model to learn from complex data in a holistic way. It breaks down into three ideas:
> * **Differentiable means there's a "paper trail."** Every operation in the pipeline---from processing the MSA to placing atoms---is a smooth mathematical function. This means the model can precisely calculate how a tiny change in any internal parameter will affect the final output.
>
> * **Backpropagation follows the trail to assign blame.** After predicting a structure, the model calculates its final error (the FAPE loss). The learning algorithm, **backpropagation**, then works backward along this paper trail. It sends a "gradient," or an error signal, that tells every single parameter in the network exactly how much it contributed to the final mistake.
>
> * **End-to-End means everyone is responsible.** Because this blame signal travels all the way to the start, the parameters in the very first Evoformer block are directly penalized for a misplaced atom in the final 3D structure. This forces the entire system to learn in concert, with the single, unified goal of producing a more accurate final structure.

<br>

![The AlphaFold2 Structure Module. (d) The module takes the final single and pair representations and uses an Invariant Point Attention (IPA) module to iteratively update a set of local reference frames for each residue. (e) These local frames define the orientation of each amino acid. (f) The final output is a complete 3D structure, shown here superimposed on the ground truth.](/images/structure_module.png)
*The AlphaFold2 Structure Module.*
<br>

##### Local frames rather than global coordinates.
At the start of round 1, every residue $i$ is assigned an *internal frame* $T_i = (R_i, \mathbf{t}_i)$: a rotation matrix $R_i \in \mathrm{SO}(3)$ and an origin $\mathbf{t}_i \in \mathbb{R}^3$. *Think of this as giving each residue its own personal compass and position tracker.* All frames are initialized to the same identity transformation in what the authors call a "black hole initialization"---the entire protein starts as a collapsed ball of overlapping atoms at the origin. This approach of operating in local frames has two key advantages:
1.  Global rigid-body motions have no effect on distances measured *within* each frame, so the network never needs to learn arbitrary coordinate conventions.
2.  Rotations and translations can be applied independently to each residue, allowing complex deformations to be accumulated over the eight cycles.

##### What makes IPA "invariant"?
Each residue advertises $K$ learnable *query points* $\mathbf{p}_{i,k} \in \mathbb{R}^3$ that live in its local frame. *These are like virtual 'attachment points' that the residue learns to place on its surface to probe its environment.* During attention these points are converted to the global frame via:
$$\tilde{\mathbf{p}}_{i,k} = R_i \mathbf{p}_{i,k} + \mathbf{t}_i.$$
*This equation simply finds the location of each attachment point in the shared, 'global' space of the entire protein.* The Euclidean distance $d_{ij,k\ell} = \lVert \tilde{\mathbf{p}}_{i,k} - \tilde{\mathbf{p}}_{j,\ell} \rVert_2$ is invariant to any simultaneous rotation or translation applied to *all* residues. *This is the core insight: because the distance between two points is a physical constant regardless of viewpoint, an attention score built from this value respects the physics of 3D space without extra hand-crafting.*

##### Scoring who to attend to.
For each head $h$, the model decides how much attention residue $i$ should pay to residue $j$. This decision isn't based on a single criterion, but on a powerful score calculated by combining three distinct sources of evidence:

1.  **An Abstract Match:** A standard attention score is calculated from the query vector of residue $i$ and the key vector of residue $j$. This term asks: "Based on our learned chemical and sequential features, are we a good match?"
2.  **A Blueprint Bias:** A powerful bias is added, which is generated directly from the final pair representation ($\mathbf{z}_{ij}$). This term asks: "Does the Evoformer's final 2D blueprint have a strong belief that we should be interacting?"
3.  **A 3D Proximity Score:** A geometric score is included based on the distance between the residues' virtual 'attachment points' in the current 3D structure. This term asks: "Given our current positions in space, are we a good geometric fit?"

The final, un-normalised attention score is the mathematical combination of these three ideas:
$$\mathrm{score}^{(h)}_{ij} = \underbrace{{\mathbf{q}^{(h)}_i}^{\mathsf{T}}\mathbf{k}^{(h)}_j}_{\text{Abstract Match}} + \underbrace{b^{(h)}_{ij}}_{\text{Blueprint Bias}} - \underbrace{\frac{1}{\sigma_h^2}\sum_{k,\ell} w^{(h)}_{k\ell} d_{ij,k\ell}^2}_{\text{3D Proximity}}.$$
*This multi-component score is incredibly powerful, as it allows the model to weigh chemical compatibility, the overall 2D plan, and the immediate 3D environment all at once when deciding which interactions are most important.* This score is calculated with learned coefficients $w_{k\ell}^{(h)}$ and a learned length scale $\sigma_h$.

##### Aggregating messages.
Softmax over $j$ produces attention weights $\alpha^{(h)}_{ij}$. Two distinct pieces of information, aggregated using these weights, are then passed back to residue~$i$:
1.  An *abstract message*, $\mathbf{m}_i = \sum_{h,j} \alpha^{(h)}_{ij} \mathbf{v}^{(h)}_j$, created by calculating a weighted sum of all other residues' 'value' vectors ($\mathbf{v}^{(h)}_j$). This message updates the residue's internal state, $\mathbf{s}_i$, with new chemical and sequential context based on the other residues the model just paid attention to.
2.  A *geometric message*---a set of averaged 'value points' in the global frame---that is converted back to the local frame of $i$ through $T_i^{-1}$. *This crucial step translates a global 'group consensus' on movement into a personal, actionable command for residue $i$*, yielding a vector $\Delta\mathbf{x}_i$ that captures where the residue "wants" to move.

##### Backbone Update.
After the IPA block has updated the single representation $\mathbf{s}_i$ with both abstract and geometric messages, the final vector is fed to a small MLP module called `BackboneUpdate`. This module's job is to translate the abstract information in $\mathbf{s}_i$ into a concrete movement command. This command consists of a translation vector, $\delta \mathbf{t}_i$, and a 3D rotation vector, $\boldsymbol\omega_i$.

The network predicts a simple 3D vector for the rotation because directly outputting the nine constrained values of a valid rotation matrix is very difficult for a neural network. Instead, it predicts an **axis-angle vector** ($\boldsymbol\omega_i$), where the vector's direction defines an axis of rotation and its length defines how much to rotate. The final rotation matrix, $\delta R_i$, is then generated using the **exponential map**, a standard mathematical function that reliably converts the simple vector command into a perfect 3x3 rotation matrix:
$$\delta R_i = \exp(\boldsymbol\omega_i)$$
*This command is a small, relative 'nudge'---a slight turn and a slight shift.* The frame is then updated by applying this nudge to its current state via composition:
$$R_i \leftarrow \delta R_i R_i, \qquad \mathbf{t}_i \leftarrow \delta R_i \mathbf{t}_i + \delta \mathbf{t}_i.$$
Repeating IPA followed by Backbone Update eight times unfolds the chain into a well-packed backbone without ever measuring absolute orientation.

##### Side-chain placement and relaxation.
A final set of MLP heads predicts the torsion angles $\{\chi_n\}$ for each residue, from which all heavy side-chain atoms are placed using standard bond lengths and angles. (For production pipelines requiring maximum accuracy, an optional AMBER energy minimisation is often run to remove residual clashes, but the network alone already achieves sub-angstrom accuracy for most backbone atoms.)

<br>

> ### Why this Design Works
> IPA gives the network three complementary signals when deciding if residues should be close: their biochemical compatibility (query–key term), the Evoformer’s experience (bias term), and the evidence of their current partial fold (distance term). The gating present in both the attention weights and the Backbone Update lets the model ignore unhelpful suggestions, preventing oscillations and speeding up convergence.

### The Training Objective: A Symphony of Losses

A neural network as complex as AlphaFold 2 cannot be trained by optimizing a single, simple objective. The genius of the model lies not only in its architecture but also in how it is taught. The training process is guided by a carefully crafted **loss function**, which is actually a weighted sum of several distinct components. This multi-faceted objective ensures that every part of the network, from the Evoformer to the Structure Module, learns its specific task effectively.

The final loss is a combination of a main structural loss and several "auxiliary" losses that provide additional supervisory signals.

##### The Main Loss: Frame Aligned Point Error (FAPE).
The primary goal is to produce a structure that is as close as possible to the ground truth. While a simple Root Mean Square Deviation (RMSD) might seem like an obvious choice, the DeepMind team created a more powerful and nuanced alternative called the **Frame Aligned Point Error (FAPE)**.

##### Auxiliary Losses for Richer Supervision.
To supplement FAPE, AlphaFold 2 uses several other loss terms to guide intermediate parts of the network:

* **Distogram Loss:** The refined pair representation in the Evoformer is used to predict a **distogram**—a 2D histogram of distances between every pair of residues. This prediction is compared against the true distogram from the experimental structure. *This loss ensures that the Evoformer's geometric reasoning is accurate even before a 3D structure is built, providing a strong intermediate supervisory signal.*

* **MSA Masking Loss:** In a technique inspired by language models like BERT (Rao et al., 2021), the model is given an MSA where some of the amino acids have been randomly hidden or "masked". The model's task is to predict the identity of these masked amino acids. *This forces the Evoformer to learn the deep statistical 'grammar' of protein evolution, strengthening its understanding of co-evolutionary patterns.*

* **Predicted Confidence Metrics:** AlphaFold 2 doesn't just predict a structure; it predicts its own accuracy by generating a suite of confidence scores. These include the per-residue **pLDDT** to assess local quality, the **Predicted Aligned Error (PAE)** to judge domain packing, and the **pTM-score** to estimate the quality of the overall global fold. *Training the model to predict its own errors is crucial for real-world utility, as it tells scientists when and how much to trust the output.*

By combining these different objectives, the model is trained to simultaneously understand evolutionary relationships (MSA masking), reason about 2D geometry (distogram), build accurate 3D structures (FAPE), and assess its own work using its internal confidence scores. This symphony of losses is a key reason for its remarkable accuracy.

##### Extension to Complexes: AlphaFold-Multimer.
While the initial model focused on single protein chains, the architecture's power was quickly extended to predict the structure of protein complexes. A fine-tuned version, **AlphaFold-Multimer**, was trained specifically on protein complexes and introduced key algorithmic tweaks to handle multiple chains. It also introduced a new confidence score, **ipTM** (interface predicted Template Modeling score), to specifically assess the confidence of the predicted protein-protein interfaces. This variant demonstrated that the core Evoformer-based architecture was capable of modeling complex assemblies, setting the stage for the even more generalized models to follow (Evans et al., 2022).

This intricate, deterministic machine set a new standard for accuracy, but its complexity paved the way for a return to the flexible, generative power of the diffusion models we first introduced.

##### A Summary of the Revolution.
Ultimately, the genius of AlphaFold 2 lies in its integrated design. It doesn't treat protein structure prediction as a simple pipeline, but as a holistic reasoning problem. The Evoformer creates a rich, context-aware blueprint by forcing a deep dialogue between evolutionary data and a geometric hypothesis. The Structure Module then uses a physically-inspired, equivariant attention mechanism to translate this abstract blueprint into a precise atomic model. This end-to-end philosophy, guided by a symphony of carefully chosen loss functions, is what allowed AlphaFold 2 to not just advance the field, but to fundamentally redefine what was thought possible.

***
# Appendix

## A Deeper Look at Self-Attention

The core computational engine of the Transformer architecture is the **self-attention mechanism**. Its purpose is to generate a new, contextually-aware representation for each element in an input sequence. It achieves this by systematically weighing the influence of every other element in the sequence, allowing the model to dynamically capture relationships between them.

To illustrate this, we will use a relevant example from protein biology. Imagine the model is processing a peptide sequence and needs to learn the electrostatic interaction between a positively charged Lysine (K) and a negatively charged Aspartate (D), which can form a salt bridge. We will walk through how self-attention enables the model to learn this relationship, focusing on updating the representation for Lysine.

1. **Input Embeddings:** We begin with an input matrix **X**, where each row **x**<sub>i</sub> is a vector (an "embedding") representing a single amino acid residue. These initial embeddings are learned and encode fundamental properties. For our simplified 3-residue sequence (Alanine, Lysine, Aspartate), let's assume a 4-dimensional embedding (d<sub>model</sub>=4). The dimensions could conceptually represent properties like size, hydrophobicity, positive charge, and negative charge.

   $$
   \mathbf{X} =
   \begin{bmatrix}
   \mathbf{x}_{\text{Ala}} \\
   \mathbf{x}_{\text{Lys}} \\
   \mathbf{x}_{\text{Asp}}
   \end{bmatrix}
   =
   \begin{bmatrix}
   0.2 & 0.1 & 0.0 & 0.0 \\ 
   0.8 & 0.4 & 1.0 & 0.0 \\ 
   0.5 & 0.3 & 0.0 & 1.0   
   \end{bmatrix}
   $$

2. **Projecting to Query, Key, and Value Spaces:** The model learns three distinct weight matrices (**W**<sub>Q</sub>, **W**<sub>K</sub>, **W**<sub>V</sub>) to project each input embedding into three different roles. This projection allows the model to extract specific features relevant for establishing relationships.

   - A **Query** vector (**q**): Asks, "Given my properties, who should I pay attention to?" For Lysine, its query vector will be trained to effectively ask, "I have a positive charge; where is a corresponding negative charge?"
   - A **Key** vector (**k**): States, "Here are the properties I offer for others to query." For Aspartate, its key vector will be trained to state, "I possess a negative charge."
   - A **Value** vector (**v**): Contains the information of that residue to be passed on to others. If a residue receives high attention, it will heavily incorporate this value vector.

   Let's define our learned weight matrices, which project from d<sub>model</sub>=4 to a smaller dimension d<sub>k</sub>=d<sub>v</sub>=2.

   $$
   \mathbf{W}_Q = \begin{bmatrix} 0 & 1 \\ 0 & 1 \\ 1 & 0 \\ 0 & 0 \end{bmatrix} \quad
   \mathbf{W}_K = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \\ 1 & 0 \end{bmatrix} \quad
   \mathbf{W}_V = \begin{bmatrix} 0 & 1 \\ 1 & 0 \\ 1 & 1 \\ 0 & 1 \end{bmatrix}
   $$

   We calculate the **Q**, **K**, and **V** matrices by multiplying our input **X** with these weight matrices.

   $$
   \begin{align}
   \mathbf{Q} &= \mathbf{X}\mathbf{W}_Q = \begin{bmatrix} 0.2 & 0.3 \\ 0.8 & 1.2 \\ 0.5 & 0.8 \end{bmatrix} \\
   \mathbf{K} &= \mathbf{X}\mathbf{W}_K = \begin{bmatrix} 0.2 & 0.1 \\ 0.8 & 0.4 \\ 1.5 & 0.3 \end{bmatrix} \\
   \mathbf{V} &= \mathbf{X}\mathbf{W}_V = \begin{bmatrix} 0.1 & 0.4 \\ 1.2 & 1.8 \\ 0.3 & 1.8 \end{bmatrix}
   \end{align}
   $$

3. **Calculate Raw Scores (Attention Logits):** To determine how much attention Lysine should pay to other residues, we take its query vector, **q**<sub>Lys</sub> = [0.8, 1.2], and compute its dot product with the key vector of every residue in the sequence. The dot product is a measure of similarity; a higher value signifies greater relevance.

   $$
   \begin{align}
   \text{score}(\text{Lys, Ala}) &= \mathbf{q}_{\text{Lys}} \cdot \mathbf{k}_{\text{Ala}} = [0.8, 1.2] \cdot [0.2, 0.1] = (0.8)(0.2) + (1.2)(0.1) = 0.28 \\
   \text{score}(\text{Lys, Lys}) &= \mathbf{q}_{\text{Lys}} \cdot \mathbf{k}_{\text{Lys}} = [0.8, 1.2] \cdot [0.8, 0.4] = (0.8)(0.8) + (1.2)(0.4) = 1.12 \\
   \text{score}(\text{Lys, Asp}) &= \mathbf{q}_{\text{Lys}} \cdot \mathbf{k}_{\text{Asp}} = [0.8, 1.2] \cdot [1.5, 0.3] = (0.8)(1.5) + (1.2)(0.3) = 1.56
   \end{align}
   $$

   As designed, the score for the interacting Lys-Asp pair is the highest, indicating a strong potential relationship.

4. **Scale and Normalize (Softmax):** The scores [0.28, 1.12, 1.56] are first scaled. This is a critical step for stabilizing training. As the dimension of the key vectors (d<sub>k</sub>) increases, the variance of the dot products also increases, which can push the softmax function into saturated regions with extremely small gradients. To counteract this, we scale the scores by dividing by √d<sub>k</sub>. Here, d<sub>k</sub>=2, so we scale by √2 ≈ 1.414.

   $$
   \text{Scaled Scores} = \left[ \frac{0.28}{\sqrt{2}}, \frac{1.12}{\sqrt{2}}, \frac{1.56}{\sqrt{2}} \right] \approx [0.198, 0.792, 1.103]
   $$

   Next, these scaled scores are passed through a softmax function, which converts them into a probability distribution of positive weights that sum to 1. These are the final **attention weights** (α).

   $$
   \begin{align}
   \alpha_{\text{Lys}} &= \text{softmax}([0.198, 0.792, 1.103]) \\
   &= \left[ \frac{e^{0.198}}{e^{0.198} + e^{0.792} + e^{1.103}}, \frac{e^{0.792}}{e^{0.198} + e^{0.792} + e^{1.103}}, \frac{e^{1.103}}{e^{0.198} + e^{0.792} + e^{1.103}} \right] \\
   &= \left[ \frac{1.22}{1.22 + 2.21 + 3.01}, \frac{2.21}{1.22 + 2.21 + 3.01}, \frac{3.01}{1.22 + 2.21 + 3.01} \right] \\
   &\approx [0.19, 0.34, \mathbf{0.47}]
   \end{align}
   $$

   The weights clearly show that to update its representation, Lysine should draw 47% of its new information from Aspartate.

5. **Produce the Final Output:** The new, context-aware representation for Lysine, denoted **z**<sub>Lys</sub>, is the weighted sum of all the **Value** vectors in the sequence, using the attention weights we just calculated.

   $$
   \begin{align}
   \mathbf{z}_{\text{Lys}} &= (\alpha_{\text{Lys,Ala}} \times \mathbf{v}_{\text{Ala}}) + (\alpha_{\text{Lys,Lys}} \times \mathbf{v}_{\text{Lys}}) + (\alpha_{\text{Lys,Asp}} \times \mathbf{v}_{\text{Asp}}) \\
   &= (0.19 \times [0.1, 0.4]) + (0.34 \times [1.2, 1.8]) + (0.47 \times [0.3, 1.8]) \\
   &= [0.019, 0.076] + [0.408, 0.612] + [0.141, 0.846] \\
   &= [0.568, 1.534]
   \end{align}
   $$

The final output vector for Lysine, **z**<sub>Lys</sub>=[0.568, 1.534], has now powerfully incorporated information from Aspartate, effectively encoding their likely interaction directly into its features. This new vector is then passed to the next layer of the Transformer.

### Multi-Head Attention

A single attention calculation (as shown above) allows the model to focus on one type of relationship at a time (e.g., electrostatic interactions). However, protein structures are governed by many concurrent interactions (hydrophobic interactions, hydrogen bonds, steric constraints, etc.).

**Multi-Head Attention** addresses this limitation by performing the entire self-attention process multiple times in parallel. Each parallel run is called a "head," and each head has its own independently learned weight matrices (**W**<sub>Q</sub><sup>(i)</sup>, **W**<sub>K</sub><sup>(i)</sup>, **W**<sub>V</sub><sup>(i)</sup>). This is analogous to having multiple specialist "heads" analyzing the sequence simultaneously:

- **Head 1** might learn to identify salt bridges.
- **Head 2** might focus on hydrophobic interactions.
- **Head 3** might track backbone hydrogen bond patterns.
- etc.

The resulting output vectors from all heads (**z**<sup>(1)</sup>, **z**<sup>(2)</sup>, ...) are concatenated and then passed through a final linear projection matrix, **W**<sub>O</sub>, to produce the final, unified output. This creates a representation that is simultaneously rich with information about many different kinds of structural and chemical relationships.
***

## References

1.  J. Jumper, R. Evans, A. Pritzel, et al. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature*, 596(7873):583–589.
2.  R. F. Service. (2020). ‘The game has changed.’ AI triumphs at solving protein structures. *Science*. [https://www.science.org/content/article/game-has-changed-ai-triumphs-solving-protein-structures](https://www.science.org/content/article/game-has-changed-ai-triumphs-solving-protein-structures)
3.  C. Outeiral Rubiera. (2021). AlphaFold 2 is here: what’s behind the structure prediction miracle. *Oxford Protein Informatics Group Blog*. [https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)
4.  A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems*, volume 30.
5.  J. Alammar. (2018). The Illustrated Transformer. *Blog Post*. [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
6.  S. Ovchinnikov. (2018). Co-evolutionary methods for protein structure prediction. *Stanford CS371 Lecture Slides*. [https://cs371.stanford.edu/2018_slides/coevolution.pdf](https://cs371.stanford.edu/2018_slides/coevolution.pdf)
7.  K. Phie. (2023). Invariant Point Attention explained. *Medium*. [https://medium.com/@kasothaphie/invariant-point-attention-explained-c71aa56f5f5c](https://medium.com/@kasothaphie/invariant-point-attention-explained-c71aa56f5f5c)
8.  R. Evans, M. O'Neill, A. Pritzel, et al. (2022). Protein complex prediction with AlphaFold-Multimer. *bioRxiv*. (Note: The user provided a Science citation, which is likely the final published version. The blog should be updated with the formal citation when available.)
9.  R. Rao, J. Meier, T. Sercu, S. Ovchinnikov, A. Rives. (2021). MSA Transformer. *Proceedings of the 38th International Conference on Machine Learning*.
{% endraw %}
