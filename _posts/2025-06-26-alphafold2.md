---
title: "From AlphaFold2 to Boltz-2: A Journey Through the Revolution of Protein Structure Prediction"
author: "Aditya Sengar"
date: 2025-06-26
---

# Introduction: A New Era in Structural Biology

The last few years have seen a revolution in structural biology, driven by the breakneck speed of advances in artificial intelligence. This post is the first in a series where we will deconstruct the key architectures that have defined this new era. Our journey will take us from the deterministic brilliance of AlphaFold 2 to the generative power of modern diffusion models like Boltz-2.

To understand where the field is going, we must first build a deep appreciation for the foundation upon which it all stands.

**In this first article, we focus entirely on AlphaFold 2**, the model that solved one of biology's 50-year-old grand challenges: the protein folding problem. We will move beyond a high-level overview to dissect its two-part engine:

- The **Evoformer** trunk, which established a rich, bidirectional dialogue between evolutionary data and a geometric hypothesis.
- The deterministic **Structure Module**, which used intricate, physically-inspired attention mechanisms to translate this hypothesis into a precise 3D structure.

By the end, you will have a solid, technically-grounded understanding of how AlphaFold 2 works, setting the stage for our exploration of the models that followed.

---

> ### Wonder Box: The Basics—Proteins, Residues, and Evolution
>
> **Proteins and Residues**  
> Think of a protein as a long, intricate necklace made of a specific sequence of beads. Each "bead" is an amino acid, which in this post is almost always called a residue. There are 20 common types of these residues, each with unique chemical properties.
>
> **The Protein Folding Problem**  
> The central challenge of structural biology is that this 1D string of residues folds itself into a single, complex, and functional 3D shape. For 50 years, predicting this final 3D shape from only the 1D sequence of residues was a grand challenge in science. This is the problem AlphaFold 2 solved.
>
> **MSA (Multiple Sequence Alignment)**  
> To get clues about the 3D shape, scientists look at the protein's "evolutionary cousins" found in other species. An MSA is a massive alignment of these related sequences. Why is this important? If two residues are touching in the 3D structure, a mutation in one is often compensated by a matching mutation in the other across many "cousin" sequences. By finding these correlated pairs of mutations (a signal called co-evolution), the model gets powerful hints about which residues are close to each other in the final structure.

---

Long before diffusion models entered the mainstream for structural biology, AlphaFold 2[^jumper2021nature] achieved a watershed moment. When DeepMind presented their results at the CASP14 competition in late 2020, they didn't just improve upon existing methods; they shattered all expectations, leaving the entire field of computational biology wondering how it was possible.[^service2020science]

When the paper and code were finally released, the "secret sauce" was revealed. It wasn't a single, magical insight into protein folding, but rather a masterpiece of deep learning[^outeiral2021opig] engineering that fundamentally reinvented how to reason about protein structure. The computational heart of this masterpiece consists of two main engines working in concert: the **Evoformer** and the **Structure Module**.

![High-level overview of the AlphaFold2 architecture. The model is conceptually divided into three stages. First, input features are generated from the primary sequence, a Multiple Sequence Alignment (MSA), and optional structural templates. Second, the Evoformer block iteratively refines an MSA representation and a pair representation in a deep communication network. Finally, the Structure Module translates the refined representations into the final 3D atomic coordinates.](AF2.png)
*Figure 1. High-level overview of AlphaFold2 architecture.*

---

## The Evoformer's Engine: The Attention Mechanism

The Evoformer's architecture is built on the now-ubiquitous transformer[^vaswani2017attention]. Its core computational tool is the attention mechanism, a powerful technique that allows the model to learn context by weighing the influence of different parts of the input data on each other (see Appendix for a numeric walk-through). When processing a specific residue, for example, it learns how much "attention" to pay to every other residue in the protein, allowing it to dynamically identify the most important relationships. For an excellent visual explanation of this concept, see Alammar[^alammar2018transformer].

## Building the Blueprint: The Representations and Templates

The self-attention mechanism is a powerful tool, but for it to work on a problem as complex as protein structure, it needs data representations that are far richer than simple word embeddings. For years, the state-of-the-art approach was to take a Multiple Sequence Alignment (MSA) and distill its evolutionary information down into a single, static 2D contact map[^contactmap]. Methods would analyze the MSA to predict which pairs of residues were likely touching, and this map of contacts would then be fed as restraints to a separate structure-folding algorithm.[^coevolution_slides]

AlphaFold 2’s paradigm shift was to avoid this premature distillation. Instead of collapsing the rich evolutionary data into a simple 2D map, it builds and refines two powerful representations in parallel, providing the perfect canvas for its attention-based Evoformer. Let’s look at how these representations are constructed.

### The MSA Representation: A Detailed Evolutionary Profile

AlphaFold 2 first clusters the MSA. The model then processes a representation of size (N_seq × N_res), where N_seq is the total number of sequences in the stack (composed of both MSA cluster representatives and structural templates) and N_res is the number of residues.

For each residue in a representative sequence, the model starts with a rich, 49-dimensional input feature vector, including:
- **Residue identity** (one-hot encoding)
- **Local structural context** (deletion profile)
- **Evolutionary context** (cluster's statistical profile)

This 49-dimensional input vector is processed by a linear layer to produce the internal MSA representation, which has 256 channels (c_m=256). This is the representation that is iteratively refined within the Evoformer.

### The Pair Representation: The Geometric Blueprint

This is a square matrix of size (N_res × N_res) with 128 channels (c_z=128) that evolves into a detailed geometric blueprint of the protein.

#### 1. Encoding Residue Pairs via Outer Sum

The initial pair information is created by simply adding two learned 128-dimensional vectors:
$$
z_{ij}^{pairs} = a_i + b_j
$$

#### 2. Encoding Relative Position

The model injects 1D sequence separation:
$$
z_{ij}^{initial} = z_{ij}^{pairs} + p_{ij}
$$
where $p_{ij}$ is a positional embedding derived from the one-hot encoding of the clipped relative distance.

---

> ### Wonder Box: Evoformer's Dialogue—A Summary
>
> The core magic of the Evoformer lies in its two-way communication stream, repeated over 48 cycles:
> - **Path 1 (MSA → Pairs):** The model analyzes evolutionary data (MSA) to find correlated mutations, updating the geometric blueprint (Pair Representation).
> - **Path 2 (Pairs → MSA):** The model uses its current 3D hypothesis (Pair Representation) to guide its search through the evolutionary data.
>
> This cycle allows the model to build confidence by finding evidence consistent across both evolutionary and geometric domains.

---

### Structural Templates: A Powerful Head Start

If homologous structures[^homologous_structures] exist in the PDB, AlphaFold 2 leverages their known geometry as a set of powerful, editable hints:

- **2D Geometric Hint for the Pair Representation:** Extracts distograms[^distogram] and integrates them using a specialized attention mechanism.
- **1D "Advisor" for the MSA Representation:** Known backbone torsion angles are concatenated directly to the MSA representation as if they were additional sequences[^template_concat].

---

## The Evoformer's Work Cycle: A Refinement Loop

The Evoformer’s power comes from repeating a sophisticated block of operations 48 times. Each pass through this block represents one full cycle of the "dialogue" between the evolutionary and geometric representations.

### Stage 1: Processing the Evolutionary Data (MSA Stack)

The block first focuses on the MSA representation to extract and refine co-evolutionary signals. This is done with a specialized MSA-specific attention mechanism.

**Axial Attention.** To handle the massive (N_seq × N_res) MSA matrix, the model doesn't compute attention over all entries at once. Instead, it "factorizes" the attention into two much cheaper, sequential steps:
- **Row-wise Gated Self-Attention.** Performed independently for each sequence (row), this step allows the model to find relationships between residues within a single sequence.
- **Column-wise Gated Self-Attention.** Performed independently for each residue position (column), this step allows the model to compare the "evidence" from all the different evolutionary sequences for that specific position in the protein.

**MSA Transition.** After the attention steps, the MSA representation passes through an MSATransition layer. This is a standard point-wise, two-layer feed-forward network (MLP) applied to every vector in the MSA representation, allowing for more complex, non-linear features to be learned.

### Stage 2: Enforcing Geometry (Pair Stack)

The block now turns to the pair representation (z), the model's geometric blueprint. At this stage, the blueprint might contain noise or local predictions that are not globally consistent. The goal of the Pair Stack is to refine this blueprint by enforcing geometric constraints across the entire structure. Its primary tool is a set of novel "triangular" operations.

**Triangular Multiplicative Update: Densifying the Geometric Graph.**
This first operation acts as a fast, "brute-force" way to strengthen local geometric signals. It considers every possible intermediate residue k for each pair (i,j) and aggregates the evidence from all of them:

$$
\text{update\_vector} = \sum_{k} ( \text{Linear}(z_{ik}) \odot \text{Linear}(z_{jk}) )
$$

It then computes a gate vector, $g_{ij} = \text{sigmoid}(\text{Linear}(z_{ij}))$, and applies the gated update: $z_{ij} \mathrel{+}= g_{ij} \odot \text{update\_vector}$.

**Triangular Self-Attention.**
The representation for an edge (i, j) acts as a "query" to selectively gather information from other edges:

- Project to Query, Key, Value. First, the model projects the pair representations into Query, Key, and Value vectors. For our query edge, we have $q_{ij} = \text{Linear}(z_{ij})$. For the edges it will attend to, we have $k_{ik} = \text{Linear}(z_{ik})$ and $v_{ik} = \text{Linear}(z_{ik})$.
- Calculate Attention Score with Triangle Bias. For each potential interaction between edge (i,j) and edge (i,k), the model calculates a score. Crucially, this score includes a learned bias that comes directly from the triangle's closing edge, $z_{jk}$:

$$
\text{score}_{ijk} = \frac{q_{ij} \cdot k_{ik}}{\sqrt{d_k}} + \text{Linear}(z_{jk})
$$

The scores for a given i,j are passed through a softmax function over all possible nodes k to get the final attention weights, $\alpha_{ijk}$. Separately, a gate vector, $g_{ij} = \text{sigmoid}(\text{Linear}(z_{ij}))$, is computed. The final output is the gated, weighted sum of all the value vectors:

$$
\text{output}_{ij} = g_{ij} \odot \sum_k \alpha_{ijk} v_{ik}
$$

This entire process is then repeated symmetrically for the "ending node," where edge (i,j) attends to all edges ending at node j.

**Pair Transition.**  
Finally, the pair stack processing concludes with a PairTransition layer. This is a standard point-wise, two-layer feed-forward network (MLP) that is applied independently to each vector $z_{ij}$ in the pair representation. This step allows the model to perform more complex transformations on the features for each pair, helping it to better process and integrate the rich information gathered from the preceding triangular updates.

### Stage 3: The Bidirectional Dialogue (Communication Hub)

The MSA and Pair stacks don't operate in isolation. The true genius of the Evoformer is how they are forced to communicate within each block, creating a virtuous cycle where a better evolutionary model informs the geometry, and a better geometric model informs the search for evolutionary clues. This dialogue happens through two dedicated pathways.

**Path 1: From MSA to Pairs (Outer Product Mean).**
For a given pair of residues (i, j), the model takes the representation for residue i and residue j from every sequence s in the MSA stack (m_{si} and m_{sj}). It projects them through two different linear layers, calculates their outer product, and then averages these resulting matrices over all sequences:

$$
\text{update for } z_{ij} = \text{Linear} \left( \text{mean}_{s} ( \text{Linear}_a(m_{si}) \otimes \text{Linear}_b(m_{sj}) ) \right)
$$

**Path 2: From Pairs to MSA (Attention Bias).**
When the model calculates the attention score between residue i and residue j within a single sequence, it doesn't just rely on comparing their query and key vectors. It adds a powerful bias that comes directly from the pair representation.

$$
\text{score}(q_{si}, k_{sj}) = \frac{q_{si} \cdot k_{sj}}{\sqrt{d_k}} + \text{Linear}(z_{ij})
$$

---

## The Structure Module: From Feature Maps to Atomic Coordinates

After 48 Evoformer iterations, the network possesses two mature tensors: a per-residue feature vector $s_i$ and a per-pair tensor $z_{ij}$. The **Structure Module** must now turn these high-level statistics into a concrete three-dimensional model. It does so through eight rounds of a custom transformer block called **Invariant Point Attention (IPA)**[^phie2023ipa] followed by a small motion model, Backbone Update. The entire pipeline is differentiable[^end_to_end_diff].

![The AlphaFold2 Structure Module. (d) The module takes the final single and pair representations and uses an Invariant Point Attention (IPA) module to iteratively update a set of local reference frames for each residue. (e) These local frames define the orientation of each amino acid. (f) The final output is a complete 3D structure, shown here superimposed on the ground truth.](structure_module.png)
*Figure 2. AlphaFold2 Structure Module.*

---

> ### Wonder Box: The Structure Module's Philosophy—Frames, Invariance, and Refinement
>
> The Structure Module translates the abstract Evoformer blueprint into a 3D model. Its design rests on three core principles:
>
> - **Local Frames:** Instead of predicting global coordinates, the model assigns a personal, local coordinate system (a "frame") to each residue. It then learns to predict small rotations and translations to apply to each frame, unfolding the protein from a collapsed point. This makes the process robust to arbitrary global rotations.
> - **Invariant Point Attention (IPA):** This custom attention mechanism decides which residues should interact by combining three signals: chemical/sequence similarity, the Evoformer's 2D blueprint, and the current 3D proximity of the residues. Because it reasons about distances between points in local frames, its logic is "invariant" to the global orientation, respecting the physics of 3D space.
> - **Iterative Refinement:** The final structure isn't built in one shot. The IPA module iterates 8 times to refine the backbone. Furthermore, the entire prediction process is often "recycled" 3-4 times, where the model's own previous output is used as an ultra-informative template to fix mistakes and improve accuracy in the next pass.

---

**Local frames rather than global coordinates.**  
At the start of round 1, every residue $i$ is assigned an internal frame $T_i = (R_i, t_i)$: a rotation matrix $R_i \in SO(3)$ and an origin $t_i \in \mathbb{R}^3$. All frames are initialized to the same identity transformation ("black hole initialization"). This approach of operating in local frames has two key advantages:

1. Global rigid-body motions have no effect on distances measured within each frame, so the network never needs to learn arbitrary coordinate conventions.
2. Rotations and translations can be applied independently to each residue, allowing complex deformations to be accumulated over the eight cycles.

**What makes IPA "invariant"?**  
Each residue advertises $K$ learnable query points $p_{i,k} \in \mathbb{R}^3$ that live in its local frame. During attention these points are converted to the global frame via:

$$
\tilde{p}_{i,k} = R_i p_{i,k} + t_i
$$

The Euclidean distance $d_{ij,k\ell} = \lVert \tilde{p}_{i,k} - \tilde{p}_{j,\ell} \rVert_2$ is invariant to any simultaneous rotation or translation applied to all residues.

**Scoring who to attend to.**  
For each head $h$, the model decides how much attention residue $i$ should pay to residue $j$ by combining three distinct sources of evidence:

1. **An Abstract Match:** A standard attention score calculated from the query and key vectors of the residues, asking: "Based on our learned chemical and sequential features, are we a good match?"
2. **A Blueprint Bias:** A powerful bias from the Evoformer's final pair representation ($z_{ij}$), asking: "Does the 2D blueprint have a strong belief that we should be interacting?"
3. **A 3D Proximity Score:** A geometric score based on the distance between the residues' virtual 'attachment points' in the current 3D structure, asking: "Given our current positions in space, are we a good geometric fit?"

These three sources of evidence are then combined into a single, un-normalized attention score, calculated with learned coefficients ($w_{k\ell}^{(h)}$) and a length scale ($\sigma_h$):

$$
\text{score}^{(h)}_{ij} = q^{(h)}_i{}^{T} k^{(h)}_j + b^{(h)}_{ij} - \frac{1}{\sigma_h^2}\sum_{k,\ell} w^{(h)}_{k\ell} d_{ij,k\ell}^2
$$

Aggregating messages. Softmax over $j$ produces attention weights $\alpha^{(h)}_{ij}$. Two distinct pieces of information, aggregated using these weights, are then passed back to residue $i$:

1. An abstract message, $m_i = \sum_{h,j} \alpha^{(h)}_{ij} v^{(h)}_j$, created by calculating a weighted sum of all other residues' value vectors ($v^{(h)}_j$). This message updates the residue's internal state, $s_i$, with new chemical and sequential context based on the other residues the model just paid attention to.
2. A geometric message—a set of averaged 'value points' in the global frame—that is converted back to the local frame of $i$ through $T_i^{-1}$, yielding a vector $\Delta x_i$ that captures where the residue "wants" to move.

**Backbone Update.**  
After the IPA block has updated the single representation $s_i$ with both abstract and geometric messages, the final vector is fed to a small MLP module called BackboneUpdate. This module's job is to translate the abstract information in $s_i$ into a concrete movement command. This command consists of a translation vector, $\delta t_i$, and a 3D rotation vector, $\omega_i$.

The network predicts a simple 3D vector for the rotation (axis-angle), and the final rotation matrix, $\delta R_i$, is generated using the exponential map[^exp_map]:

$$
\delta R_i = \exp(\omega_i)
$$

The frame is then updated:

$$
R_i \leftarrow \delta R_i R_i \\
t_i \leftarrow \delta R_i t_i + \delta t_i
$$

Repeating IPA followed by Backbone Update eight times unfolds the chain into a well-packed backbone without ever measuring absolute orientation.

---

> ### Wonder Box: Why this Design Works
>
> IPA gives the network three complementary signals when deciding if residues should be close: their biochemical compatibility (query–key term), the Evoformer’s experience (bias term), and the evidence of their current partial fold (distance term). The gating present in both the attention weights and the Backbone Update lets the model ignore unhelpful suggestions, preventing oscillations and speeding up convergence.

---

**Side-chain placement**

After the backbone has settled, a final set of MLP heads predicts the side-chain conformations. AlphaFold 2 does this elegantly by treating side chains not as individual atoms, but as a series of connected rigid groups. For each residue, the network uses the final single representation ($s_i$) to predict up to four torsion angles ($\{\chi_1, \ldots, \chi_4\}$) that orient these rigid groups. To handle the circular nature of angles, the network actually predicts the sine and cosine of each angle ($\sin\chi_n, \cos\chi_n$) rather than the angle itself, which makes the loss function smooth. A particularly clever feature is how the loss function handles symmetric residues (like Tyrosine or Aspartate); it allows the network to be correct if it predicts either the true torsion angle or one that is rotated by 180 degrees, as both are physically equivalent.

Once these angles are predicted, all heavy side-chain atoms are placed sequentially using standard, idealized bond lengths and angles and conditionally relaxed[^relaxation].

---

## The Training Objective: A Symphony of Losses

> ### Wonder Box: Measuring Success—Key Metrics & Loss Functions
>
> - **RMSD (Root Mean Square Deviation):** The classic way to measure the similarity between two 3D structures. You optimally superimpose the predicted structure onto the true one and then calculate the average distance between their corresponding atoms. A lower RMSD means a better prediction.
> - **FAPE (Frame Aligned Point Error):** AlphaFold 2's primary loss function and a clever improvement over RMSD. Instead of one global superposition, FAPE measures the error of all atoms from the perspective of every single residue's local frame. This heavily penalizes local errors—like incorrect bond angles—that RMSD might miss. It effectively asks for every residue: "From my point of view, are all the other atoms where they should be?"
> - **Distogram (Distance Histogram):** A 2D plot where the pixel at position (i, j) isn't a single distance, but a full probability distribution across a set of distance "bins." The distogram loss forces the Evoformer's Pair Representation to learn this detailed geometric information.

A neural network as complex as AlphaFold 2 cannot be trained by optimizing a single, simple objective. The genius of the model lies not only in its architecture but also in its carefully crafted loss function, which is a weighted sum of several distinct components. This multi-faceted objective ensures that every part of the network learns its specific task effectively.

The main loss is the **Frame Aligned Point Error (FAPE)**, which measures the accuracy of the final 3D coordinates. To ensure all parts of the model are learning correctly, several "auxiliary" losses provide richer supervision:

- **Distogram Loss:** The refined pair representation in the Evoformer is used to predict a distogram. This prediction is compared against the true distogram from the experimental structure. *This loss ensures that the Evoformer's geometric reasoning is accurate even before a 3D structure is built, providing a strong intermediate supervisory signal.*
- **MSA Masking Loss:** In a technique inspired by language models like BERT, the model is given an MSA where some of the amino acids have been randomly hidden or "masked". The model's task is to predict the identity of these masked amino acids. *This forces the Evoformer to learn the deep statistical `grammar' of protein evolution, strengthening its understanding of co-evolutionary patterns.*
- **Predicted Confidence Metrics:** AlphaFold 2 doesn't just predict a structure; it predicts its own accuracy by generating a suite of confidence scores. These include the per-residue **pLDDT** to assess local quality, the **Predicted Aligned Error (PAE)** to judge domain packing, and the **pTM-score** to estimate the quality of the overall global fold. *Training the model to predict its own errors is crucial for real-world utility, as it tells scientists when and how much to trust the output.*

By combining these different objectives, the model is trained to simultaneously understand evolutionary relationships (MSA masking), reason about 2D geometry (distogram), build accurate 3D structures (FAPE), and assess its own work using its internal confidence scores. This symphony of losses is a key reason for its remarkable accuracy.

---

## A Summary of the Revolution

Ultimately, the genius of AlphaFold 2 lies in its integrated design. It doesn't treat protein structure prediction as a simple pipeline, but as a holistic reasoning problem. The Evoformer creates a rich, context-aware blueprint by forcing a deep dialogue between evolutionary data and a geometric hypothesis. The Structure Module then uses a physically-inspired, equivariant attention mechanism to translate this abstract blueprint into a precise atomic model. This end-to-end philosophy, guided by a symphony of carefully chosen loss functions, is what allowed AlphaFold 2 to not just advance the field, but to fundamentally redefine what was thought possible.

---

# Appendix: A Deeper Look at Self-Attention

The core computational engine of the Transformer architecture is the self-attention mechanism. Its purpose is to generate a new, contextually-aware representation for each element in an input sequence. It achieves this by systematically weighing the influence of every other element in the sequence, allowing the model to dynamically capture relationships between them.

To illustrate this, we will use a relevant example from protein biology. Imagine the model is processing a peptide sequence and needs to learn the electrostatic interaction between a positively charged Lysine (K) and a negatively charged Aspartate (D), which can form a salt bridge. We will walk through how self-attention enables the model to learn this relationship, focusing on updating the representation for Lysine.

1. **Input Embeddings:**  
   We begin with an input matrix $X$, where each row $x_i$ is a vector (an "embedding") representing a single amino acid residue. These initial embeddings are learned and encode fundamental properties. For our simplified 3-residue sequence (Alanine, Lysine, Aspartate), let's assume a 4-dimensional embedding ($d_{model}=4$). The dimensions could conceptually represent properties like size, hydrophobicity, positive charge, and negative charge.

   $$
   X =
   \begin{bmatrix}
   0.2 & 0.1 & 0.0 & 0.0 \\\\
   0.8 & 0.4 & 1.0 & 0.0 \\\\
   0.5 & 0.3 & 0.0 & 1.0
   \end{bmatrix}
   $$

2. **Projecting to Query, Key, and Value Spaces:**  
   The model learns three distinct weight matrices ($W_Q$, $W_K$, $W_V$) to project each input embedding into three different roles.

   $$
   W_Q = \begin{bmatrix} 0 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \end{bmatrix}, \quad
   W_K = \begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 1 & 0 \end{bmatrix}, \quad
   W_V = \begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \end{bmatrix}
   $$

   We calculate the $Q$, $K$, and $V$ matrices by multiplying our input $X$ with these weight matrices.

3. **Calculate Raw Scores (Attention Logits):**  
   To determine how much attention Lysine should pay to other residues, we take its query vector, $q_{Lys} = [0.8, 1.2]$, and compute its dot product with the key vector of every residue in the sequence.

   $$
   \begin{align*}
   \text{score}(Lys, Ala) &= [0.8, 1.2] \cdot [0.2, 0.1] = 0.28 \\\\
   \text{score}(Lys, Lys) &= [0.8, 1.2] \cdot [0.8, 0.4] = 1.12 \\\\
   \text{score}(Lys, Asp) &= [0.8, 1.2] \cdot [1.5, 0.3] = 1.56
   \end{align*}
   $$

4. **Scale and Normalize (Softmax):**  
   The scores $[0.28, 1.12, 1.56]$ are first scaled by dividing by $\sqrt{d_k}$ and then softmaxed.

   $$
   \text{Scaled Scores} = \left[ \frac{0.28}{\sqrt{2}}, \frac{1.12}{\sqrt{2}}, \frac{1.56}{\sqrt{2}} \right] \approx [0.198, 0.792, 1.103]
   $$

   $$
   \alpha_{Lys} = \text{softmax}([0.198, 0.792, 1.103]) \approx [0.19, 0.34, 0.47]
   $$

   The weights show that to update its representation, Lysine should draw 47% of its new information from Aspartate.

5. **Produce the Final Output:**  
   The new, context-aware representation for Lysine, $z_{Lys}$, is the weighted sum of all the Value vectors in the sequence, using the attention weights just calculated.

   $$
   z_{Lys} = (0.19 \times [0.1, 0.4]) + (0.34 \times [1.2, 1.8]) + (0.47 \times [0.3, 1.8]) = [0.568, 1.534]
   $$

The final output vector for Lysine, $z_{Lys} = [0.568, 1.534]$, has now powerfully incorporated information from Aspartate, effectively encoding their likely interaction directly into its features. This new vector is then passed to the next layer of the Transformer.

---

### Multi-Head Attention

A single attention calculation (as shown above) allows the model to focus on one type of relationship at a time. However, protein structures are governed by many concurrent interactions (hydrophobic interactions, hydrogen bonds, steric constraints, etc.).

**Multi-Head Attention** addresses this limitation by performing the entire self-attention process multiple times in parallel. Each parallel run is called a "head," and each head has its own independently learned weight matrices. The resulting output vectors from all heads are concatenated and then passed through a final linear projection matrix, $W_O$, to produce the final, unified output. This creates a representation that is simultaneously rich with information about many different kinds of structural and chemical relationships.

---

# References

[^service2020science]: Service, R. F. (2020). ‘The game has changed.’ AI triumphs at solving protein structures. *Science*.
[^outeiral2021opig]: Outeiral Rubiera, C. (2021). AlphaFold 2 is here: what’s behind the structure prediction miracle. *Oxford Protein Informatics Group Blog*. [link](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/)
[^jumper2021nature]: Jumper, J., Evans, R., Pritzel, A., et al. (2021). Highly accurate protein structure prediction with AlphaFold. *Nature*, 596(7873):583–589.
[^vaswani2017attention]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems*, 30.
[^alammar2018transformer]: Alammar, J. (2018). The Illustrated Transformer. [Blog Post](http://jalammar.github.io/illustrated-transformer/).
[^coevolution_slides]: Ovchinnikov, S. (2018). Co-evolutionary methods for protein structure prediction. *Stanford CS371 Lecture Slides*. [link](https://cs371.stanford.edu/2018_slides/coevolution.pdf)
[^phie2023ipa]: Phie, K. (2023). Invariant Point Attention explained. *Medium*. [link](https://medium.com/@kasothaphie/invariant-point-attention-explained-c71aa56f5f5c)
[^contactmap]: A contact map is a 2D matrix where entry (i, j) represents if residue i and residue j are in physical contact (e.g., within 8 Å).
[^homologous_structures]: Homologous structures are experimentally solved 3D structures from proteins that are evolutionarily related to the target protein.
[^distogram]: A distogram is a matrix where entry (i,j) is a probability distribution over distance bins (e.g., 39 bins from 3.25 Å to >50.75 Å).
[^template_concat]: Template torsion angle features are stacked with the main MSA tensor along the sequence dimension.
[^exp_map]: The exponential map converts an axis-angle vector into a valid rotation matrix.
[^end_to_end_diff]: End-to-end differentiable means every model parameter can be adjusted directly from the prediction error, all the way from final output to the start.
[^relaxation]: An optional final energy minimization step (e.g., with AMBER) can be used to resolve atomic clashes, but has little effect on AlphaFold’s confidence scores.
